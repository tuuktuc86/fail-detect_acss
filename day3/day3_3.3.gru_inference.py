# train_ssl_3to1_11in_8out.py
import argparse, os, math
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

IN_DIM  = 11
OUT_DIM = 8
PAST_N  = 3

class NPZNextStepDataset(Dataset):
    def __init__(self, npz_path: str):
        self.path  = npz_path
        # í‚¤/ê¸¸ì´ë§Œ 1íšŒ ì½ê³  ë‹«ê¸°
        with np.load(self.path, allow_pickle=False) as f:
            self.keys = sorted(f.files)
            self.lens = {k: f[k].shape[0] for k in self.keys}

        # ì¸ë±ìŠ¤ êµ¬ì¶• (target = t+1)
        self.index = []
        for k in self.keys:
            T = self.lens[k]
            if T >= 2:
                for t in range(0, T-1):
                    self.index.append((k, t+1))

        # ì •ê·œí™” í†µê³„ (í•œ ë²ˆë§Œ ì—´ì–´ ê³„ì‚°)
        with np.load(self.path, allow_pickle=False) as f:
            all_obs = np.concatenate([f[k] for k in self.keys], axis=0).astype(np.float32)
        self.mean_in  = torch.from_numpy(all_obs.mean(0))
        self.std_in   = torch.from_numpy(all_obs.std(0) + 1e-6)
        self.mean_out = self.mean_in[:OUT_DIM].clone()
        self.std_out  = self.std_in[:OUT_DIM].clone()

    def __len__(self): return len(self.index)

    def __getitem__(self, i):
        k, tgt = self.index[i]  # tgt in [1..T-1]
        # ì•ˆì „í•˜ê²Œ ë§¤ í˜¸ì¶œ ì‹œ íŒŒì¼ ì—´ê¸°
        with np.load(self.path, allow_pickle=False) as f:
            ep = f[k]  # (T,11)

            # ìµœê·¼ 3ìŠ¤í… ë³µì œ íŒ¨ë”©
            frames = []
            for dt in range(PAST_N, 0, -1):
                idx = tgt - dt
                if idx < 0: idx = 0
                frames.append(ep[idx])

            x = torch.from_numpy(np.stack(frames, axis=0)).float()   # (3,11)
            y = torch.from_numpy(ep[tgt, :OUT_DIM]).float()          # (8,)
        return x, y
class GRUForecast(nn.Module):
    def __init__(self, hidden=256, layers=2):
        super().__init__()
        self.gru = nn.GRU(input_size=IN_DIM, hidden_size=hidden, num_layers=layers, batch_first=True)
        self.head = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, OUT_DIM),
        )
    def forward(self, x):                 # x: (B,3,11)
        h, _ = self.gru(x)                # (B,3,H)
        return self.head(h[:, -1, :])     # (B,8)

from collections import deque
import torch

class ObsHistory:
    def __init__(self, hist_len=3):
        self.hist_len = hist_len
        self.buffer = deque(maxlen=hist_len)

    def add(self, obs):
        """obs: torch.Tensor (11,) ê°™ì€ ë‹¨ì¼ observation"""
        self.buffer.append(obs.detach().clone())

    def get(self):
        """(hist_len, obs_dim) tensor ë°˜í™˜ (ë³µì œ íŒ¨ë”© í¬í•¨)"""
        if len(self.buffer) == 0:
            raise ValueError("No obs in buffer yet!")
        
        if len(self.buffer) == 1:
            # ê°™ì€ obs 3ê°œ
            out = [self.buffer[0]] * self.hist_len
        elif len(self.buffer) == 2:
            # ì²« obs + ë§ˆì§€ë§‰ obs ë³µì œ 2ê°œ
            out = [self.buffer[0], self.buffer[1], self.buffer[1]]
        else:
            # ì´ë¯¸ 3ê°œ ì´ìƒ â†’ deque íŠ¹ì„±ìƒ ìµœê·¼ 3ê°œë§Œ ìœ ì§€ë¨
            out = list(self.buffer)
        
        return torch.stack(out, dim=0)   # (3, obs_dim)


@torch.no_grad()
def predict_next8(obs_hist_3, ckpt_path, cpu=False):
    ckpt = torch.load(ckpt_path, map_location="cpu")
    cfg = ckpt["cfg"]
    device = torch.device("cpu" if cpu else ("cuda" if torch.cuda.is_available() else "cpu"))
    model = GRUForecast(hidden=cfg["hidden"], layers=cfg["layers"]).to(device)
    model.load_state_dict(ckpt["model"]); model.eval()

    # ğŸ”¥ statsëŠ” ë°”ë¡œ torch.tensorë¡œ ë³€í™˜
    stats = np.load(os.path.join(cfg["out"], "norm_stats.npz"))
    mi = torch.tensor(stats["mean_in"], dtype=torch.float32, device=device)
    si = torch.tensor(stats["std_in"], dtype=torch.float32, device=device)
    mo = torch.tensor(stats["mean_out"], dtype=torch.float32, device=device)
    so = torch.tensor(stats["std_out"], dtype=torch.float32, device=device)

    # ì…ë ¥ obs_hist_3 (3,11)
    x = torch.as_tensor(obs_hist_3, dtype=torch.float32, device=device).unsqueeze(0)  # (1,3,11)
    x = (x - mi) / si
    y = model(x)[0]
    y = y * so + mo
    return y.cpu().numpy()




import argparse
import os
import datetime
import glob
import random
import torch
import cv2
import numpy as np
from scipy.spatial.transform import Rotation as R
import clip
from torchvision import transforms as T
from torchvision.transforms import functional as F
import gymnasium as gym
from collections.abc import Sequence
import open3d as o3d
# o3d.visualization.Visualizer().destroy_window() #rendering X 
import matplotlib.pyplot as plt
from PIL import Image

# Isaac Lab ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from isaaclab.app import AppLauncher

# Argparseë¡œ CLI ì¸ì íŒŒì‹± ë° Omniverse ì•± ì‹¤í–‰
parser = argparse.ArgumentParser(description="Tutorial on creating an empty stage.")
parser.add_argument(
    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
)

AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()
# args_cli.headless = True
# args_cli.enable_cameras = True
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

from isaaclab_tasks.utils.parse_cfg import parse_env_cfg
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils.math import subtract_frame_transforms
import omni.kit.viewport.utility as viewport_utils

# AILAB-summer-school-2025/cgnet í´ë”ì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ì‹œìŠ¤í…œ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Detection ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
import torchvision

# Contact-GraspNet ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from cgnet.utils.config import cfg_from_yaml_file
from cgnet.tools import builder
from cgnet.inference_cgnet import inference_cgnet

# ì¹´ë©”ë¼ ë Œë”ë§ ì˜µì…˜ --enable_cameras flag ë¥¼ ëŒ€ì‹ í•˜ê¸° ìœ„í•¨
import carb
carb_settings_iface = carb.settings.get_settings()
carb_settings_iface.set_bool("/isaaclab/cameras_enabled", True)

# ì»¤ìŠ¤í…€ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ config íŒŒì¼ ì„í¬íŠ¸
from task.lift.custom_pickplace_env_cfg_3_3 import YCBPickPlaceEnvCfg

# gymnasium ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì„ ì–¸
from task.lift.config.ik_abs_env_cfg_3_3 import FrankaYCBPickPlaceEnvCfg
gym.register(
    id="Isaac-Lift-Cube-Franka-Custom-v0",
    entry_point="isaaclab.envs:ManagerBasedRLEnv",
    kwargs={
        "env_cfg_entry_point": FrankaYCBPickPlaceEnvCfg,
    },
    disable_env_checker=True,
)


# Detection ëª¨ë¸ ì„¤ì •
DIR_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(DIR_PATH, 'data/checkpoint/maskrcnn_ckpt/maskrcnn_trained_model_refined.pth') # <-- ì‚¬ì „ í•™ìŠµëœ Weight
NUM_CLASSES = 79  # ëª¨ë¸ êµ¬ì¡°ëŠ” í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨
CONFIDENCE_THRESHOLD = 0.5

YCB_OBJECT_CLASSES = sorted([
    '002_master_chef_can', '008_pudding_box', '014_lemon', '021_bleach_cleanser', '029_plate', '036_wood_block', '044_flat_screwdriver', '054_softball', '061_foam_brick', '065_c_cups', '065_i_cups', '072_b_toy_airplane', '073_c_lego_duplo',
    '003_cracker_box', '009_gelatin_box', '015_peach', '022_windex_bottle', '030_fork', '037_scissors', '048_hammer', '055_baseball', '062_dice', '065_d_cups', '065_j_cups', '072_c_toy_airplane', '073_d_lego_duplo',
    '004_sugar_box', '010_potted_meat_can', '016_pear', '024_bowl', '031_spoon', '038_padlock', '050_medium_clamp', '056_tennis_ball', '063_a_marbles', '065_e_cups', '070_a_colored_wood_blocks', '072_d_toy_airplane', '073_e_lego_duplo',
    '005_tomato_soup_can', '011_banana', '017_orange', '025_mug', '032_knife', '040_large_marker', '051_large_clamp', '057_racquetball', '063_b_marbles', '065_f_cups', '070_b_colored_wood_blocks', '072_e_toy_airplane', '073_f_lego_duplo',
    '006_mustard_bottle', '012_strawberry', '018_plum', '026_sponge', '033_spatula', '042_adjustable_wrench', '052_extra_large_clamp', '058_golf_ball', '065_a_cups', '065_g_cups', '071_nine_hole_peg_test', '073_a_lego_duplo', '073_g_lego_duplo',
    '007_tuna_fish_can', '013_apple', '019_pitcher_base', '028_skillet_lid', '035_power_drill', '043_phillips_screwdriver', '053_mini_soccer_ball', '059_chain', '065_b_cups', '065_h_cups', '072_a_toy_airplane', '073_b_lego_duplo', '077_rubiks_cube'
])
CLASS_NAME = ['BACKGROUND'] + YCB_OBJECT_CLASSES

def depth2pc(depth, K, rgb=None):
    """ ëìŠ¤ ì´ë¯¸ì§€ë¥¼ í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
    
    mask = np.where(depth > 0)
    x, y = mask[1], mask[0]
    
    normalized_x = (x.astype(np.float32)-K[0,2])
    normalized_y = (y.astype(np.float32)-K[1,2])
    
    world_x = normalized_x * depth[y, x] / K[0,0]
    world_y = normalized_y * depth[y, x] / K[1,1]
    world_z = depth[y, x]
    
    if rgb is not None:
        rgb = rgb[y, x]
    
    pc = np.vstack([world_x, world_y, world_z]).T
    return (pc, rgb)

def get_world_bbox(depth, K, bb): 
    """ Bounding Boxì˜ ì¢Œí‘œë¥¼ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ê¸°ì¤€ ì¢Œí‘œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """

    image_width = depth.shape[1]
    image_height = depth.shape[0]

    x_min, x_max = bb[0], bb[2]
    y_min, y_max = bb[1], bb[3]
    
    if y_min < 0:
        y_min = 0
    if y_max >= image_height:
        y_max = image_height-1
    if x_min < 0:
        x_min = 0
    if x_max >=image_width:
        x_max = image_width-1

    z_0, z_1 = depth[int(y_min), int(x_min)], depth[int(y_max), int(x_max)]
    
    def to_world(x, y, z):
        """ ëìŠ¤ í¬ì¸íŠ¸ë¥¼ 3D í¬ì¸íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
        world_x = (x - K[0, 2]) * z / K[0, 0]
        world_y = (y - K[1, 2]) * z / K[1, 1]
        return world_x, world_y, z
    
    x_min_w, y_min_w, z_min_w = to_world(x_min, y_min, z_0)
    x_max_w, y_max_w, z_max_w = to_world(x_max, y_max, z_1)
    
    return x_min_w, y_min_w, x_max_w, y_max_w

def get_model_instance_segmentation(num_classes):
    """ í•™ìŠµ ë•Œì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ Mask R-CNN ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤. """
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=None)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)
    return model

def get_random_color():
    """ ì‹œê°í™”ë¥¼ ìœ„í•´ ëœë¤ RGB ìƒ‰ìƒì„ ìƒì„±í•©ë‹ˆë‹¤. """
    return [random.randint(50, 255) for _ in range(3)] # ë„ˆë¬´ ì–´ë‘¡ì§€ ì•Šì€ ìƒ‰ìƒ

def CLIP_transform(img_tensor, output_size, fill=0, padding_mode='constant'):
    """ CLIP ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ë³€í™˜ """

    # ê¸´ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆë  í¬ê¸° ê³„ì‚°
    _, h, w = img_tensor.shape
    if h > w:
        new_h = output_size
        new_w = int(w * (new_h / h))
    else: # w >= h
        new_w = output_size
        new_h = int(h * (new_w / w))

    # ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
    img_tensor = F.resize(img_tensor, [new_h, new_w])

    # ëª©í‘œ í¬ê¸°ì— ë§ê²Œ íŒ¨ë”©ì„ ì¶”ê°€ (left, top, right, bottom)
    pad_left = (output_size - new_w) // 2
    pad_top = (output_size - new_h) // 2
    pad_right = output_size - new_w - pad_left
    pad_bottom = output_size - new_h - pad_top
    padding = (pad_left, pad_top, pad_right, pad_bottom)

    # íŒ¨ë”©ì„ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜
    padded_img = F.pad(img_tensor, padding, fill, padding_mode)

    # mean/stdë¥¼ ì‚¬ìš©í•´ ì •ê·œí™”
    normalized_img = F.normalize(padded_img.to(torch.float32), 
                                 mean=[0.48145466, 0.4578275, 0.40821073], 
                                 std=[0.26862954, 0.26130258, 0.27577711])
    
    return normalized_img

from PIL import Image
# trajectory ëë‚œ í›„ log_dir ì•ˆì˜ front_view_* ì´ë¯¸ì§€ë¥¼ gifë¡œ ë¬¶ê¸°



class GripperState:
    """ ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ ê·¸ë¦¬í¼ state ì •ì˜ """
    OPEN = 1.0
    CLOSE = -1.0

class PickAndPlaceSmState:
    """ ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ  ìƒí™© state ì •ì˜ """
    REST = 0
    PREDICT = 1
    READY = 2
    PREGRASP = 3
    GRASP = 4
    CLOSE = 5
    LIFT = 6
    MOVE_TO_BIN = 7
    LOWER = 8
    RELEASE = 9
    BACK = 10
    BACK_TO_READY = 11

class PickAndPlaceSmWaitTime:
    """ ê° pick-and-place ìƒí™© state ë³„ ëŒ€ê¸° ì‹œê°„(ì´ˆ) ì •ì˜ """
    REST = 3.0
    PREDICT = 0.0
    READY = 0.5
    PREGRASP = 1.0
    GRASP = 0.5
    CLOSE = 1.0
    LIFT = 0.5
    MOVE_TO_BIN = 0.5
    LOWER = 0.5
    RELEASE = 0.5
    BACK = 0.5
    BACK_TO_READY = 0.5


class PickAndPlaceSm:
    """
    ë¡œë´‡ì´ ë¬¼ì²´ë¥¼ ì§‘ì–´ ì˜®ê¸°ëŠ”(Pick-and-Place) ì‘ì—…ì„ ìƒíƒœë¨¸ì‹ (State Machine)ìœ¼ë¡œ êµ¬í˜„.
    ê° ë‹¨ê³„ë³„ë¡œ End-Effector ìœ„ì¹˜ì™€ ê·¸ë¦¬í¼ ìƒíƒœë¥¼ ì§€ì •í•´ì¤Œ.

    0. REST: ë¡œë´‡ì´ ì´ˆê¸°ìì„¸ ìƒíƒœì— ìˆìŠµë‹ˆë‹¤.
    1. PREDICT: íŒŒì§€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    2. READY: ë¡œë´‡ì´ ì´ˆê¸°ìì„¸ ìƒíƒœì— ìœ„ì¹˜í•˜ê³ , ê·¸ë¦¬í¼ë¥¼ CLOSE ìƒíƒœë¡œ ë‘¡ë‹ˆë‹¤.
    3. PREGRASP: íƒ€ê²Ÿ ë¬¼ì²´ ì•ìª½ì˜ pre-grasp ìì„¸ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    4. GRASP: ì—”ë“œì´í™í„°ë¥¼ íƒ€ê²Ÿ ë¬¼ì²´ì— grasp ìì„¸ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.
    5. CLOSE: ê·¸ë¦¬í¼ë¥¼ ë‹«ì•„ ë¬¼ì²´ë¥¼ ì§‘ìŠµë‹ˆë‹¤.
    6. LIFT: ë¬¼ì²´ë¥¼ ë“¤ì–´ì˜¬ë¦½ë‹ˆë‹¤.
    7. MOVE_TO_BIN: ë¬¼ì²´ë¥¼ ëª©í‘œ xy ìœ„ì¹˜(ë°”êµ¬ë‹ˆ)ë¡œ ì´ë™ì‹œí‚¤ê³ , ë†’ì´ë„ íŠ¹ì • ë†’ì´ê¹Œì§€ ìœ ì§€í•©ë‹ˆë‹¤.
    8. LOWER: ë¬¼ì²´ë¥¼ ë‚®ì€ z ìœ„ì¹˜ê¹Œì§€ ë‚´ë¦½ë‹ˆë‹¤.
    9. RELEASE: ê·¸ë¦¬í¼ë¥¼ ì—´ì–´ ë¬¼ì²´ë¥¼ ë†“ìŠµë‹ˆë‹¤.
    10. BACK: ì—”ë“œì´í™í„°ë¥¼ ë°”êµ¬ë‹ˆ ìœ„ì˜ íŠ¹ì • ë†’ì´ë¡œ ë‹¤ì‹œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    11. BACK_TO_READY: ì—”ë“œì´í™í„°ë¥¼ ì›ë˜ ì´ˆê¸° ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    """
    def __init__(self, dt: float, num_envs: int, device: torch.device | str = "cpu", position_threshold=0.01):
        """Initialize the state machine.

        Args:
            dt: The environment time step.
            num_envs: The number of environments to simulate.
            device: The device to run the state machine on.
        """
        # state machine íŒŒë¼ë¯¸í„° ê°’(1)
        self.dt = float(dt)
        self.num_envs = num_envs
        self.device = device
        self.position_threshold = position_threshold

        # state machine íŒŒë¼ë¯¸í„° ê°’(2)
        self.sm_dt = torch.full((self.num_envs,), self.dt, device=self.device)
        self.sm_state = torch.full((self.num_envs,), 0, dtype=torch.int32, device=self.device)
        self.sm_wait_time = torch.zeros((self.num_envs,), device=self.device)

        # ëª©í‘œ ë¡œë´‡ ëë‹¨(end-effector) ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ
        self.des_ee_pose = torch.zeros((self.num_envs, 7), device=self.device)
        self.des_gripper_state = torch.full((self.num_envs, 1), 0.0, device=self.device)

        # ë¬¼ì²´ ì´ë¯¸ì§€ë¥¼ ì·¨ë“í•˜ê¸° ìœ„í•œ ì¤€ë¹„ ìì„¸
        self.ready_pose = torch.tensor([[ 3.0280e-01, -5.6916e-02,  6.2400e-01, -1.4891e-10,  1.0000e+00, 8.4725e-11, -8.7813e-10]], device=self.device)  # (x, y, z, qw, qx, qy, qz)
        self.ready_pose = self.ready_pose.repeat(num_envs, 1)

        # ë¬¼ì²´ë¥¼ ìƒìì— ë‘ê¸° ìœ„í•´ ìƒì ìœ„ì— ìœ„ì¹˜í•˜ëŠ” ìì„¸
        self.bin_pose = torch.tensor([[ 0.2, 0.6, 0.55, 0, 1, 0, 0]], device=self.device)   # (x, y, z, qw, qx, qy, qz)
        self.bin_pose = self.bin_pose.repeat(num_envs, 1)

        # ë¬¼ì²´ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìƒìì— ë‘ê¸° ìœ„í•œ ë‚®ì€ ìì„¸
        self.bin_lower_pose = torch.tensor([[ 0.2, 0.6, 0.35, 0, 1, 0, 0]], device=self.device)   # (x, y, z, qw, qx, qy, qz)
        self.bin_lower_pose = self.bin_lower_pose.repeat(num_envs, 1)

        # Contact-GraspNet ì¶”ë¡  ê°’ì„ ë‹´ê¸°ìœ„í•œ ë³€ìˆ˜ ì„ ì–¸
        self.grasp_pose = torch.zeros((self.num_envs, 7), device=self.device)
        self.pregrasp_pose = torch.zeros((self.num_envs, 7), device=self.device)

        # Gripperê°€ ì›í•˜ëŠ” ìœ„ì¹˜ì— ë„ë‹¬í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°, statemachineì´ ë©ˆì¶”ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ ì„ ì–¸
        self.stack_ee_pose = []
        self.noise_start_step = [None for _ in range(self.num_envs)]

    # env idx ë¥¼ í†µí•œ reset ìƒíƒœ ì‹¤í–‰
    def reset_idx(self, env_ids: Sequence[int] | None = None):
        if env_ids is None:
            env_ids = slice(None)
        self.sm_state[env_ids] = PickAndPlaceSmState.REST
        self.sm_wait_time[env_ids] = 0.0

    ##################################### State Machine #####################################
    # ë¡œë´‡ì˜ end-effector ë° ê·¸ë¦¬í¼ì˜ ëª©í‘œ ìƒíƒœ ê³„ì‚°
    def compute(self, ee_pose: torch.Tensor, grasp_pose: torch.Tensor, pregrasp_pose: torch.Tensor, robot_data, current_step):
        ee_pos = ee_pose[:, :3]
        ee_pos[:, 2] -= 0.5

        # ê° environmentì— ë°˜ë³µì ìœ¼ë¡œ ì ìš©
        for i in range(self.num_envs):
            state = self.sm_state[i]
            # ê° ìƒíƒœì— ë”°ë¥¸ ë¡œì§ êµ¬í˜„
            if state == PickAndPlaceSmState.REST:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.ready_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.REST:
                    # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                    self.sm_state[i] = PickAndPlaceSmState.PREDICT
                    self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.PREDICT:
                # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                self.sm_state[i] = PickAndPlaceSmState.READY
                self.sm_wait_time[i] = 0.0
                
            elif state == PickAndPlaceSmState.READY:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.ready_pose[i]
                self.des_gripper_state[i] = GripperState.CLOSE
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.READY:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.PREGRASP
                        self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.PREGRASP:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = pregrasp_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # í˜„ì¬ stateì—ì„œì˜ end-effector positionì„ ì €ì¥
                self.stack_ee_pose.append(ee_pos[i])
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.PREGRASP:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.GRASP
                        self.sm_wait_time[i] = 0.0
                # end-effectorì˜ ìœ„ì¹˜ê°€ ì¼ì • step ì´ìƒ ë°”ë€Œì§€ ì•Šì„ë•Œ, ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                else:
                    if len(self.stack_ee_pose) > 50:
                        if torch.linalg.norm(ee_pos[i] - self.stack_ee_pose[-30]) < self.position_threshold:
                            self.sm_state[i] = PickAndPlaceSmState.CLOSE
                            self.sm_wait_time[i] = 0.0
                            self.stack_ee_pose = []

            elif state == PickAndPlaceSmState.GRASP:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = grasp_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # í˜„ì¬ stateì—ì„œì˜ end-effector positionì„ ì €ì¥
                self.stack_ee_pose.append(ee_pos[i])
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.GRASP:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.CLOSE
                        self.sm_wait_time[i] = 0.0
                        self.stack_ee_pose = []
                # end-effectorì˜ ìœ„ì¹˜ê°€ ì¼ì • step ì´ìƒ ë°”ë€Œì§€ ì•Šì„ë•Œ, ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                else:
                    if len(self.stack_ee_pose) > 50:
                        if torch.linalg.norm(ee_pos[i] - self.stack_ee_pose[-30]) < self.position_threshold:
                            self.sm_state[i] = PickAndPlaceSmState.CLOSE
                            self.sm_wait_time[i] = 0.0
                            self.stack_ee_pose = []

            elif state == PickAndPlaceSmState.CLOSE:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = ee_pose[i]
                self.des_gripper_state[i] = GripperState.CLOSE
                # íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.CLOSE:
                    # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                    self.sm_state[i] = PickAndPlaceSmState.LIFT
                    self.sm_wait_time[i] = 0.0
                    # ì¼ì • ë†’ì´ë¡œ ë“¤ì–´ ì˜¬ë¦´ ìœ„ì¹˜ ì„¤ì •
                    self.lift_pose = grasp_pose[i]
                    self.lift_pose[2] = self.lift_pose[2] + 0.4

            elif state == PickAndPlaceSmState.LIFT:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.lift_pose 
                self.des_gripper_state[i] = GripperState.CLOSE
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.LIFT:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.MOVE_TO_BIN
                        self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.MOVE_TO_BIN:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.bin_pose[i]

                # # ##[failcase4] change mis put to bin 
                # if self.noise_start_step[i] is None:
                #     self.noise_start_step[i]= current_step
                #     print(f"[INFO] Noise first added at step {current_step}")
                # self.test_noise = torch.tensor([
                #     np.random.uniform(-0.6, 0.1),  
                #     np.random.uniform(-0.7, 0.1), 
                #     0.0
                # ], device=self.bin_pose.device)
                # self.des_ee_pose[i, :3] += self.test_noise    
                # # ## ----------------------

                self.des_gripper_state[i] = GripperState.CLOSE
                # í˜„ì¬ stateì—ì„œì˜ end-effector positionì„ ì €ì¥
                self.stack_ee_pose.append(ee_pos[i])
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.MOVE_TO_BIN:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.LOWER
                        self.sm_wait_time[i] = 0.0
                        self.stack_ee_pose = []
                # end-effectorì˜ ìœ„ì¹˜ê°€ ì¼ì • step ì´ìƒ ë°”ë€Œì§€ ì•Šì„ë•Œ, ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                else:
                    if len(self.stack_ee_pose) > 50:
                        if torch.linalg.norm(ee_pos[i] - self.stack_ee_pose[-30]) < self.position_threshold:                       
                            self.sm_state[i] = PickAndPlaceSmState.CLOSE
                            self.sm_wait_time[i] = 0.0
                            self.stack_ee_pose = []
                self.position_threshold = 0.01

            elif state == PickAndPlaceSmState.LOWER:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.bin_lower_pose[i]
                self.des_gripper_state[i] = GripperState.CLOSE
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.LOWER:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.RELEASE
                        self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.RELEASE:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.bin_lower_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.RELEASE:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.BACK
                        self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.BACK:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.bin_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.BACK:
                        # ë‹¤ìŒ state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.BACK_TO_READY
                        self.sm_wait_time[i] = 0.0

            elif state == PickAndPlaceSmState.BACK_TO_READY:
                # ëª©í‘œ end-effector ìì„¸ ë° ê·¸ë¦¬í¼ ìƒíƒœ ì •ì˜
                self.des_ee_pose[i] = self.ready_pose[i]
                self.des_gripper_state[i] = GripperState.OPEN
                # ëª©í‘œìì„¸ ë„ë”œì‹œ íŠ¹ì • ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
                if torch.linalg.norm(ee_pos[i] - self.des_ee_pose[i, :3]) < self.position_threshold:
                    if self.sm_wait_time[i] >= PickAndPlaceSmWaitTime.BACK_TO_READY:
                        # ë‚¨ì€ ë¬¼ì²´ë¥¼ ì¡ê¸° ìœ„í•´, PREDICT state ë¡œ ì „í™˜ ë° state ì‹œê°„ ì´ˆê¸°í™”
                        self.sm_state[i] = PickAndPlaceSmState.PREDICT
                        self.sm_wait_time[i] = 0.0
                        
            # state machine ë‹¨ìœ„ì‹œê°„ ê²½ê³¼
            self.sm_wait_time[i] += self.dt

            actions = torch.cat([self.des_ee_pose, self.des_gripper_state], dim=-1)

        return actions
    ###############################################################################################

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í™˜ê²½ ê°¯ìˆ˜(1ê°œë¡œ ê³ ì •)
    num_envs = 1

    # í™˜ê²½ ë° ì„¤ì • íŒŒì‹±
    env_cfg: YCBPickPlaceEnvCfg = parse_env_cfg(
        "Isaac-Lift-Cube-Franka-Custom-v0",
        device=args_cli.device,
        num_envs=num_envs,
        use_fabric=not args_cli.disable_fabric,
    )
    hist = ObsHistory(hist_len=3)

    user_text = None

    # í™˜ê²½ ìƒì„± ë° ì´ˆê¸°í™”
    env = gym.make("Isaac-Lift-Cube-Franka-Custom-v0", cfg=env_cfg)
    
    # Get selected object names from the environment config
    # from task.lift.custom_pickplace_env_cfg_3_3 import SELECTED_OBJECT_NAMES
    # selected_names = SELECTED_OBJECT_NAMES
    
    # # Generate a random permutation for picking order (e.g., if selected_names = ['1', '2', '3'], might become ['1', '3', '2'])
    # pick_order = selected_names.copy()
    # random.shuffle(pick_order)
    # chosen_selected_names = pick_order.copy()  # Copy for saving (original shuffled order)
    
    env.reset()
    print(f"Environment reset. Number of environments: {env.unwrapped.num_envs}")
    
    # í™˜ê²½ ê´€ì¸¡ ì¹´ë©”ë¼ ì‹œì  ì…‹íŒ…
    env.unwrapped.sim.set_camera_view(eye=[2.0, 0.0, 1.5], target=[0.0, 0.0, 0.5]) # person view 
    #env.unwrapped.sim.set_camera_view(eye=[0.0, 0.0, 5.0], target=[0.0, 0.0, 0.0])  # top view 

    from isaacsim.core.api.objects import DynamicCuboid
    from isaacsim.sensors.camera import Camera
    from isaacsim.core.api import World
    import isaacsim.core.utils.numpy.rotations as rot_utils
    import numpy as np
    import matplotlib.pyplot as plt
    
    camera = Camera(
        prim_path="/World/camera",
        position=np.array([3.67, 0.218, 1.0]),
        frequency=50,
        resolution=(640, 480),
        orientation = rot_utils.euler_angles_to_quats(np.array([0, 88, 90]), degrees=True),)
    camera.set_world_pose(np.array([3.7245, 0.218, 1.053]), rot_utils.euler_angles_to_quats(np.array([88, 0, 90]), degrees=True), camera_axes="usd")
    camera.initialize()
    camera.add_motion_vectors_to_frame()

    camera2 = Camera(
        prim_path="/World/camera2",
        position=np.array([0.16304, 0.4922, 5]),
        frequency=50,
        resolution=(640, 480),
        orientation = rot_utils.euler_angles_to_quats(np.array([0, 0, -90]), degrees=True),)
    camera2.set_world_pose(np.array([0.16304, 0.4922, 5]), rot_utils.euler_angles_to_quats(np.array([0, 0, -90]), degrees=True), camera_axes="usd")
    camera2.initialize()
    camera2.add_motion_vectors_to_frame()

    # í™˜ê²½ ì—°ì‚° ë””ë°”ì´ìŠ¤(gpu)
    device = env.unwrapped.scene.device

    # Detection ëª¨ë¸ ë¡œë“œ
    detection_model = get_model_instance_segmentation(NUM_CLASSES)
    detection_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    detection_model.eval()
    detection_model.to(device)

    # Contact-GraspNet ëª¨ë¸ configë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ê²½ë¡œ ì„¤ì •
    grasp_model_config_path = os.path.join(DIR_PATH, 'cgnet/configs/config.yaml')
    grasp_model_config = cfg_from_yaml_file(grasp_model_config_path)

    # Contact-GraspNet ëª¨ë¸ ì„ ì–¸ ë° checkpoint ì…ë ¥ì„ í†µí•œ ëª¨ë¸ weight ë¡œë“œ
    grasp_model = builder.model_builder(grasp_model_config.model)
    grasp_model_path = os.path.join(DIR_PATH, 'data/checkpoint/contact_grasp_ckpt/ckpt-iter-60000_gc6d.pth')
    builder.load_model(grasp_model, grasp_model_path)
    grasp_model.to(device)
    grasp_model.eval()

    # CLIP ëª¨ë¸ ë¡œë“œ
    clip_model, preprocess = clip.load("ViT-B/32", device='cpu')

    print("[INFO]: Setup complete...")

    # ë¡œë´‡ pick-and-place ì œì–´ë¥¼ ìœ„í•œ State machine ì„ ì–¸
    pick_and_place_sm = PickAndPlaceSm(
        dt=env_cfg.sim.dt * env_cfg.decimation,
        num_envs=num_envs,
        device=device,
        position_threshold=0.01
    )

    # í™˜ê²½ì—ì„œ robot handeye camera ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
    robot_camera = env.unwrapped.scene.sensors['camera']
    robot_camera2 = env.unwrapped.scene.sensors['camera2']

    # ì¹´ë©”ë¼ ì¸íŠ¸ë¦°ì‹(intrinsics)
    K = robot_camera.data.intrinsic_matrices.squeeze().cpu().numpy()
    
    # Create a directory for saving logs
    # log_dir = f"simulation_logs_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    # os.makedirs(log_dir, exist_ok=True)    
    max_steps_per_traj=500
    max_trajectories=50
    total_traj = 0
    dataset = {
        "EE_pose": [],
        "obs": [],
        "applied_torque": [],
    }
    # ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„
    while simulation_app.is_running() and total_traj < max_trajectories:
        env.reset()
        pick_and_place_sm.reset_idx()
        print(f"[INFO] Starting trajectory {total_traj+1}/{max_trajectories}")
        
        done = False
        step_count = 0
        save_count=0
        is_success = False
        dataset = {
            "EE_pose": [],
            "obs": [],
            "applied_torque": [],
        }
        # ìš°ì„  success/failure ë¼ë²¨ ì—†ëŠ” í´ë” ìƒì„±
        log_dir = f"simulation_traj_{total_traj}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(os.path.join(log_dir, "front_view"), exist_ok=True)
        os.makedirs(os.path.join(log_dir, "wrist_view"), exist_ok=True)
        os.makedirs(os.path.join(log_dir, "top_view"), exist_ok=True)
        while not done and step_count < max_steps_per_traj:
        # ëª¨ë¸ ì¶”ë¡  ìƒíƒœ - í•™ìŠµ ì—°ì‚° ë¹„í™œì„±í™”
            with torch.no_grad(): #with torch.inference_mode():
                # env ë³„ ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„ ì‹¤í–‰
                for env_num in range(num_envs):
                    # í˜„ì¬ stateê°€ Precdictì¼ë•Œ, Detection-GraspPrediction-CLIP ìˆœìœ¼ë¡œ ì¶”ë¡  ì§„í–‰
                    if pick_and_place_sm.sm_state[env_num] == PickAndPlaceSmState.PREDICT:
                        # ì‹œê°í™”ë¥¼ ìœ„í•œ RGB ì´ë¯¸ì§€ ë° Depth ì´ë¯¸ì§€ ì–»ê¸°

                        image_ = robot_camera.data.output["rgb"][env_num]
                        image = image_.permute(2, 0, 1)  # (channels, height, width)
                        img_np = image_.detach().cpu().numpy()

                        # Continue with depth processing if needed
                        normalized_image = (image - image.min()) / (image.max() - image.min())
                        depth = robot_camera.data.output["distance_to_image_plane"][env_num]
                        depth_np = depth.squeeze().detach().cpu().numpy()                    # ì·¨ë“í•œ Depth ì´ë¯¸ì§€ë¥¼ í†µí•œ Point Cloud ìƒì„±
                        if num_envs > 1:

                            pc, _ = depth2pc(depth_np, K[env_num])
                        else:
                            pc, _ = depth2pc(depth_np, K)

                ############################ Detection Model Inference ############################
                        print("Running detection inference...")

                        # Detection ëª¨ë¸ ì¶”ë¡ 
                        with torch.no_grad():
                            prediction = detection_model([normalized_image])
                        
                        # ê²°ê³¼ í›„ì²˜ë¦¬ ë° ì‹œê°í™”
                        img_np = cv2.cvtColor(np.array(img_np), cv2.COLOR_RGB2BGR)
                        
                        # Bboxì™€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë¦´ ì´ë¯¸ì§€ ë ˆì´ì–´
                        img_with_boxes = img_np.copy()

                        # ë§ˆìŠ¤í¬ë¥¼ ê·¸ë¦´ íˆ¬ëª…í•œ ì´ë¯¸ì§€ ë ˆì´ì–´
                        mask_overlay = img_np.copy()
                        
                        # predictionì—ì„œ pred_scores, pred_boxes, pred_masks, pred_labels ê°’ì„ ì¶”ì¶œ
                        pred_scores = prediction[0]['scores'].cpu().numpy()
                        pred_boxes = prediction[0]['boxes'].cpu().numpy()
                        pred_masks = prediction[0]['masks'].cpu().numpy()
                        pred_labels = prediction[0]['labels'].cpu().numpy()

                        print(f"Found {len(pred_scores)} objects. Visualizing valid results...")

                        # Detection ê²°ê³¼ë¥¼ rgb ì´ë¯¸ì§€ ìœ„ì— í‘œì‹œ
                        # ê° ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ í¬ë¡­ëœ ì´ë¯¸ì§€ ì €ì¥ìš©
                        crop_images = []
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì €ì¥ìš©
                        bboxes = []
                        for i in range(len(pred_scores)):
                            score = pred_scores[i]
                            label_id = pred_labels[i]

                            # ì‹ ë¢°ë„ì™€ ë ˆì´ë¸” IDë¥¼ í•¨ê»˜ í™•ì¸í•˜ì—¬ Background ì œì™¸
                            if score > CONFIDENCE_THRESHOLD and label_id != 0:
                                color = get_random_color()
                                
                                # --- Bboxì™€ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ---
                                box = pred_boxes[i]
                                x1, y1, x2, y2 = map(int, box)
                                #cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 2)
                                
                                class_names = CLASS_NAME
                                label_text = f"{class_names[label_id]}: {score:.2f}"
                                cv2.putText(img_with_boxes, label_text, (x1, y1 - 10), 
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                                
                                # --- ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸° ---
                                mask = pred_masks[i, 0]
                                binary_mask = (mask > 0.5) # Boolean mask
                                # ë§ˆìŠ¤í¬ ì˜ì—­ì—ë§Œ ìƒ‰ìƒ ì ìš©
                                mask_overlay[binary_mask] = color

                                # --- ë°”ìš´ë”© ë°•ìŠ¤ì™€ í¬ë¡­ëœ ì´ë¯¸ì§€ ì €ì¥ ---
                                bboxes.append(box)
                                crop_image = image[:, int(y1):int(y2), int(x1):int(x2)]
                                crop_images.append(crop_image)

                        # Bboxì™€ Maskë¥¼ ë¶„ë¦¬í•´ì„œ ê·¸ë¦° í›„ ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ í•©ì„±
                        alpha = 0.5 # ë§ˆìŠ¤í¬ íˆ¬ëª…ë„
                        #final_result = cv2.addWeighted(mask_overlay, alpha, img_with_boxes, 1 - alpha, 0)

                        # pltë¡œ í•œë²ˆì— ë³´ì—¬ì£¼ê¸°
                        # if crop_images:
                        #     # ì´ ì´ë¯¸ì§€ ê°œìˆ˜: final_result + cropëœ ì´ë¯¸ì§€ë“¤
                        #     num_total_images = 1 + len(crop_images)
                        #     cols = 4  # í•œ ì¤„ì— ë³´ì—¬ì¤„ ì´ë¯¸ì§€ ìˆ˜
                        #     rows = (num_total_images + cols - 1) // cols
                        #     fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))
                        #     fig.suptitle('Detection Results', fontsize=16)

                        #     # axesê°€ 1ì°¨ì› ë°°ì—´ì´ ë˜ë„ë¡ ì¡°ì •
                        #     if rows * cols == 1:
                        #         axes = [axes]
                        #     else:
                        #         axes = axes.flatten()

                        #     # ì²«ë²ˆì§¸ subplotì— final_result ì´ë¯¸ì§€ í‘œì‹œ
                        #     #final_result_rgb = cv2.cvtColor(final_result, cv2.COLOR_BGR2RGB)
                        #     # axes[0].imshow(final_result_rgb)
                        #     # axes[0].set_title('Robot View with Detections')
                        #     # axes[0].axis('off')

                        #     # ë‘ë²ˆì§¸ subplotë¶€í„° cropëœ ì´ë¯¸ì§€ë“¤ í‘œì‹œ
                        #     valid_preds_count = 0
                        #     for i in range(len(pred_scores)):
                        #         score = pred_scores[i]
                        #         label_id = pred_labels[i]

                        #         if score > CONFIDENCE_THRESHOLD and label_id != 0:
                        #             ax = axes[valid_preds_count + 1]
                                    
                        #             # # ì´ë¯¸ì§€ Display
                        #             # img_to_show = crop_images[valid_preds_count].permute(1, 2, 0).cpu().numpy()
                        #             # # ax.imshow(img_to_show)
                                    
                        #             # # ê° ì´ë¯¸ì§€ì˜ Class ì´ë¦„ í‘œì‹œ
                        #             # title = f"{CLASS_NAME[label_id]}\nScore: {score:.2f}"
                        #             # ax.set_title(title)
                        #             # ax.axis('off')
                                    
                        #             valid_preds_count += 1

                        #     # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” subplots ë„ê¸°
                        #     for i in range(num_total_images, len(axes)):
                        #         axes[i].axis('off')

                        #     # Layout ì¡°ì • ë° ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
                        #     plt.tight_layout(rect=[0, 0.03, 1, 0.95])
                        #     plt.savefig("data/detection_result.png")

                ####################################################################################

                ############################ CLIP Model Inference ##################################
                        # ê° crop image ë³„ input textì™€ì˜ ìœ ì‚¬ë„ ì €ì¥
                        probs =  []
                        #user_text = pick_order.pop(0)
                        user_text = "bowl" #sys.stdin.readline().strip()
                        
                        for crop_image in crop_images:
                            # textë¥¼ deep learning ëª¨ë¸ì— ë„£ê¸° ìœ„í•´ tokenìœ¼ë¡œ ë³€í™˜
                            text = clip.tokenize([user_text]).to(device)
                            
                            # imageë¥¼ clip modelì˜ input sizeë¡œ ë³€í™˜
                            crop_image = CLIP_transform(crop_image, 224)
                            crop_image = crop_image.unsqueeze(0)
                            
                            # CLIP ëª¨ë¸ ì¶”ë¡   ë° ìœ ì‚¬ë„ ì €ì¥ (í˜„ì¥ ê°•ì˜ ì»´í“¨í„° ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ cpu ì—°ì‚°)
                            with torch.no_grad():
                                logits_per_image, logits_per_text = clip_model(crop_image.cpu(), text.cpu())
                                probs.append(logits_per_image.cpu().numpy())
                                
                        # crop image ì¤‘ì—ì„œ input textì™€ ê°€ì¥ ìœ ì‚¬ë„ê°€ í° ì´ë¯¸ì§€ ì„ íƒ
                        target_obj_idx = np.argmax(np.array(probs))
                        target_obj_bbox = bboxes[target_obj_idx]
                        target_image = crop_images[target_obj_idx].permute(1, 2, 0).cpu().numpy()
                        
                        # BGR ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ 
                        target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB)
                        
                        # CLIP model ê²°ê³¼ cv2ë¡œ ì €ì¥
                        #cv2.imwrite(f'data/CLIP_result_{user_text}.png', target_image)

                ####################################################################################

                ############################ Grasp Model Inference ##################################
                        # targe objectì˜ bbox ìœ„ì¹˜ë¥¼ image ì¢Œí‘œì—ì„œ world ì¢Œí‘œë¡œ ë³€í™˜
                        if num_envs > 1:
                            x_min_w, y_min_w, x_max_w, y_max_w = get_world_bbox(depth_np, K[env_num], target_obj_bbox)
                        else:
                            x_min_w, y_min_w, x_max_w, y_max_w = get_world_bbox(depth_np, K, target_obj_bbox)

                        # Robotì˜ end-effector ìœ„ì¹˜ ì–»ê¸°
                        robot_entity_cfg = SceneEntityCfg("robot", body_names=["panda_hand"])
                        robot_entity_cfg.resolve(env.unwrapped.scene)
                        hand_body_id = robot_entity_cfg.body_ids[0]
                        hand_pose_w = env.unwrapped.scene["robot"].data.body_state_w[:, hand_body_id, :]  # (num_envs, 13)

                        if pc is not None:
                            offset = 0.08 # ë°”ë‹¥ì´ ë„ˆë¬´ ì¡°ê¸ˆ ë‚˜ì˜¬ ê²½ìš°, ë°”ë‹¥ì— íŒŒì§€ì ì´ ìƒê¹€
                            # target objectê°€ ìˆëŠ” ë¶€ë¶„ì˜ point cloudë¥¼ world bbox ê¸°ì¤€ìœ¼ë¡œ offsetì„ ì£¼ê³  crop
                            pc = pc[pc[:, 0] > x_min_w-offset]
                            pc = pc[pc[:, 0] < x_max_w+offset]
                            pc = pc[pc[:, 1] > y_min_w-offset]
                            pc = pc[pc[:, 1] < y_max_w+offset]
                            pc = pc[pc[:, 2] > 0.4]  # z
                            
                            # target objectì˜ 3d point cloud ì‹œê°í™”
                            # pc_o3d = o3d.geometry.PointCloud()
                            # pc_o3d.points = o3d.utility.Vector3dVector(pc)
                            # o3d.visualization.draw_geometries([pc_o3d])
                            
                            # Contact-GraspNet ëª¨ë¸ ì¶”ë¡ 
                            rot_ee, trans_ee, width = inference_cgnet(pc, grasp_model, device, hand_pose_w, env)
                            print(f"[INFO] Received ee coordinates from inference_cgnet")
                            print(f"[INFO] Gripper width: {width}")
                            
                            # ì˜ˆì¸¡í•œ íŒŒì§€ì ì„ Isaaclab í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (rotation matrix -> quat)
                            grasp_rot = rot_ee
                            pregrasp_pos = trans_ee
                            grasp_quat = R.from_matrix(grasp_rot).as_quat()  # (x, y, z, w)
                            grasp_quat = np.array([grasp_quat[3], grasp_quat[0], grasp_quat[1], grasp_quat[2]]) # (w, x, y, z)
                            
                            # rotation matrixë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•œ íŒŒì§€ì ì˜ offset ë§ì¶”ê¸°
                            z_axis = grasp_rot[:, 2]
                            grasp_pos = pregrasp_pos + z_axis * 0.085 # change : miss put point  

                            # ì˜ˆì¸¡í•œ íŒŒì§€ì  poseë¥¼ torch tensorë¡œ ë³€í™˜
                            pregrasp_pose = np.concatenate([pregrasp_pos, grasp_quat])
                            grasp_pose = np.concatenate([grasp_pos, grasp_quat])
                            pregrasp_pose = torch.tensor(pregrasp_pose, device=device).unsqueeze(0)
                            grasp_pose = torch.tensor(grasp_pose, device=device).unsqueeze(0)

                            # State machine ì— grasp ë° pregrasp ìì„¸ ì—…ë°ì´íŠ¸
                            pick_and_place_sm.grasp_pose[env_num] = grasp_pose[0]
                            pick_and_place_sm.pregrasp_pose[env_num] = pregrasp_pose[0]
                ####################################################################################

                

                # ë¡œë´‡ì˜ End-Effector ìœ„ì¹˜ì™€ ìì„¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ actions ê³„ì‚°
                robot_data = env.unwrapped.scene["robot"].data
                env.unwrapped.scene["robot"].data.applied_torque[0, -2:] = 0.0
                ee_frame_sensor = env.unwrapped.scene["ee_frame"]
                tcp_rest_position = ee_frame_sensor.data.target_pos_w[..., 0, :].clone() - env.unwrapped.scene.env_origins
                tcp_rest_orientation = ee_frame_sensor.data.target_quat_w[..., 0, :].clone()
                ee_pose = torch.cat([tcp_rest_position, tcp_rest_orientation], dim=-1)
                imitation_obs = torch.cat([ee_pose[0], pick_and_place_sm.des_gripper_state[0], env.unwrapped.unwrapped.scene.state['rigid_object']['object_0']['root_pose'][0][:3]], dim=0)
                hist.add(imitation_obs)
                # print("step = ", save_count)
                # print(env.unwrapped.unwrapped.scene.state['rigid_object']['object_0']['root_pose'])
                # print(ee_pose)
                # print(pick_and_place_sm.des_gripper_state)
                # imitation_obs = torch.cat([ee_pose[0], pick_and_place_sm.des_gripper_state[0], env.unwrapped.unwrapped.scene.state['rigid_object']['object_0']['root_pose'][0][:3]], dim=0)   # (1,14)
                # print(imitation_obs.shape)
#env.unwrapped.unwrapped.scene.state['rigid_object']['object_0']['root_pose']
                # state machine ì„ í†µí•œ action ê°’ ì¶œë ¥
                actions = pick_and_place_sm.compute(
                    ee_pose=ee_pose,
                    grasp_pose=pick_and_place_sm.grasp_pose,
                    pregrasp_pose=pick_and_place_sm.pregrasp_pose,
                    robot_data=robot_data,
                    current_step= step_count, 
                )  
                #print("ee_pose = ", ee_pose)
                # í™˜ê²½ì— ëŒ€í•œ ì•¡ì…˜ì„ ì‹¤í–‰
                
                
                
                #print("step_count = ", step_count)
                
                #print(f"obs = ", obs)
                if pick_and_place_sm.sm_state >=1: #after predict
                    
                    
                    #print("save_count = ", save_count)
                    if save_count % 5 == 0:
                        dataset["EE_pose"].append(ee_pose[0])
                        dataset["obs"].append(obs['policy'][0])
                        dataset["applied_torque"].append(robot_data.applied_torque[0])  
                        rgb_image = camera.get_rgba()
                        if rgb_image.shape[0] != 0:
                            rgb = rgb_image[:, :, :3]
                            if rgb.max() <= 1.0:
                                rgb = (rgb * 255).astype(np.uint8)
                            else:
                                rgb = rgb.astype(np.uint8)
                            img_pil_ = Image.fromarray(rgb)
                            # img_pil_.save(os.path.join(log_dir, f'front_view_{freq}.png'))
                            new_w, new_h = img_pil_.size[0] // 2, img_pil_.size[1] // 2
                            img_pil_resized = img_pil_.resize((new_w, new_h), Image.Resampling.LANCZOS)

                            img_pil_resized.save(os.path.join(log_dir, f'front_view/front_view_{save_count}.png'))

                        rgb_image = camera2.get_rgba()
                        if rgb_image.shape[0] != 0:
                            rgb = rgb_image[:, :, :3]
                            if rgb.max() <= 1.0:
                                rgb = (rgb * 255).astype(np.uint8)
                            else:
                                rgb = rgb.astype(np.uint8)
                            img_pil_ = Image.fromarray(rgb)
                            #img_pil_.save(os.path.join(log_dir, f'top_view_{freq}.png'))
                            new_w, new_h = img_pil_.size[0] // 2, img_pil_.size[1] // 2
                            img_pil_resized = img_pil_.resize((new_w, new_h), Image.Resampling.LANCZOS)

                            img_pil_resized.save(os.path.join(log_dir, f'top_view/top_view_{save_count}.png'))

                        # Save robot_camera2 image (im2)
                        im2 = robot_camera2.data.output['rgb'][env_num]
                        if len(im2.shape) == 3:  # Expected shape: (channels, height, width) or (height, width, channels)
                            if im2.shape[0] in [3, 4]:  # If channels-first (e.g., (3, height, width))
                                im2 = im2.permute(1, 2, 0)  # Convert to (height, width, channels)
                            im2_np = im2.detach().cpu().numpy()
                            if im2_np.max() <= 1.0:
                                im2_np = (im2_np * 255).astype(np.uint8)
                            else:
                                im2_np = im2_np.astype(np.uint8)
                            img_pil = Image.fromarray(im2_np)
                            new_w, new_h = img_pil.size[0] // 2, img_pil.size[1] // 2
                            img_pil_resized = img_pil.resize((new_w, new_h), Image.Resampling.LANCZOS)
                            img_pil_resized.save(os.path.join(log_dir, f'wrist_view/wrist_view_{save_count}.png'))                       
                            #img_pil.save(os.path.join(log_dir, f'wrist_view_{freq}.png'))
                    save_count+=1
                    # # Save viewport image
                    # vp = viewport_utils.get_active_viewport()
                    # viewport_utils.capture_viewport_to_file(vp, os.path.join(log_dir, f'viewport_{freq}.png'))
                    # Concatenate states into a dictionary and save as .npz (multi-array file)
                    # robotstate = torch.cat([ee_pose, torch.tensor(obs["policy"][0, 7:9].tolist(), device=ee_pose.device).unsqueeze(0)], dim=-1).cpu().numpy()
                    # data_dict = {
                    #     'robot_state': robotstate,
                    # }
                    # np.savez(os.path.join(log_dir, f'states_{step_count}.npz'), **data_dict)

                obs_hist_3= hist.get()
                pred_next = predict_next8(obs_hist_3, "/AILAB-summer-school-2025/next8/best.pt")
                print(pred_next)
                obs, rewards, terminated, truncated, info = env.step(pred_next)
                print(actions)
                # print("===================")
                # print(ee_pose[0])
                # print(obs['policy'][0])
                # print(robot_data.applied_torque)
                step_count += 1

                # ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ì—¬ë¶€ ì²´í¬
                dones = terminated | truncated
                if dones:
                    done = True
                    if terminated:
                        is_success = True
                        print("Episode terminated")
                    else:
                        print("Episode truncated")

        #print(f"eepose len = {len(dataset['EE_pose'])}, obs len = {len(dataset['obs'])}, applied_torque len = {len(dataset['applied_torque'])}")
        for k in dataset:
            dataset[k] = np.array([
                v.detach().cpu().numpy() if isinstance(v, torch.Tensor) else v
                for v in dataset[k]
            ])
        np.savez(os.path.join(log_dir, "robot_state.npz"), **dataset)
        traj_length = save_count
        if is_success:
            final_log_dir = f"{log_dir}_len{traj_length}_success"
        else:
            noise_starting_point = pick_and_place_sm.noise_start_step[env_num]
            if noise_starting_point is not None:
                final_log_dir = f"{log_dir}_len{traj_length}_failure_{noise_starting_point}step"
            else:
                final_log_dir = f"{log_dir}_len{traj_length}_failure"
        os.rename(log_dir, final_log_dir)
        
        print(f"[INFO] Trajectory {total_traj+1} saved at {final_log_dir}")
        total_traj += 1 
    # í™˜ê²½ ì¢…ë£Œ ë° ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ
    env.close()
    simulation_app.close()       
# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    main()