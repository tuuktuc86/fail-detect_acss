{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2369062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode000', 'episode001', 'episode002', 'episode003', 'episode004', 'episode005', 'episode006', 'episode007', 'episode008', 'episode009', 'episode010', 'episode011', 'episode012', 'episode013', 'episode014', 'episode015', 'episode016', 'episode017', 'episode018', 'episode019', 'episode020', 'episode021', 'episode022', 'episode023', 'episode024', 'episode025', 'episode026', 'episode027', 'episode028', 'episode029', 'episode030', 'episode031', 'episode032', 'episode033', 'episode034', 'episode035', 'episode036', 'episode037', 'episode038', 'episode039', 'episode040', 'episode041', 'episode042', 'episode043', 'episode044', 'episode045', 'episode046', 'episode047', 'episode048', 'episode049', 'episode050', 'episode051', 'episode052', 'episode053', 'episode054', 'episode055', 'episode056', 'episode057', 'episode058', 'episode059', 'episode060', 'episode061', 'episode062', 'episode063', 'episode064', 'episode065', 'episode066', 'episode067', 'episode068', 'episode069', 'episode070', 'episode071', 'episode072', 'episode073', 'episode074', 'episode075', 'episode076', 'episode077', 'episode078', 'episode079', 'episode080', 'episode081', 'episode082', 'episode083', 'episode084', 'episode085', 'episode086', 'episode087', 'episode088', 'episode089', 'episode090', 'episode091', 'episode092', 'episode093', 'episode094', 'episode095', 'episode096', 'episode097', 'episode098', 'episode099', 'episode100', 'episode101', 'episode102', 'episode103', 'episode104', 'episode105', 'episode106', 'episode107', 'episode108']\n",
      "(318, 11)\n",
      "[ 3.0280012e-01 -5.6915589e-02  1.1275961e+00  6.6436871e-07\n",
      "  9.9999994e-01 -3.4745012e-06  3.8833377e-06  1.0000000e+00\n",
      "  4.2091480e-01  2.2506712e-02  5.0053000e-01]\n",
      "[ 3.0280003e-01 -5.6915559e-02  1.1275964e+00  7.0907276e-07\n",
      "  9.9999988e-01 -3.2411465e-06  3.8638618e-06  1.0000000e+00\n",
      "  4.2091480e-01  2.2506712e-02  5.0053000e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"/AILAB-summer-school-2025/dataset_all.npz\")\n",
    "print(data.files)                # ['episode001', 'episode002']\n",
    "print(data[\"episode000\"].shape)  # (5, 11)\n",
    "print(data[\"episode000\"][0])\n",
    "print(data[\"episode000\"][2])\n",
    "#print(data[\"episode002\"].shape)  # (5, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e548ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile '/AILAB-summer-school-2025/dataset_all.npz' with keys: episode000, episode001, episode002, episode003, episode004..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load(\"/AILAB-summer-school-2025/dataset_all.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272c1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode000', 'episode001', 'episode002', 'episode003', 'episode004', 'episode005', 'episode006', 'episode007', 'episode008', 'episode009', 'episode010', 'episode011', 'episode012', 'episode013', 'episode014', 'episode015', 'episode016', 'episode017', 'episode018', 'episode019', 'episode020', 'episode021', 'episode022', 'episode023', 'episode024', 'episode025', 'episode026', 'episode027', 'episode028', 'episode029', 'episode030', 'episode031', 'episode032', 'episode033', 'episode034', 'episode035', 'episode036', 'episode037', 'episode038', 'episode039', 'episode040', 'episode041', 'episode042', 'episode043', 'episode044', 'episode045', 'episode046', 'episode047', 'episode048', 'episode049', 'episode050', 'episode051', 'episode052', 'episode053', 'episode054', 'episode055', 'episode056', 'episode057', 'episode058', 'episode059', 'episode060', 'episode061', 'episode062', 'episode063', 'episode064', 'episode065', 'episode066', 'episode067', 'episode068', 'episode069', 'episode070', 'episode071', 'episode072', 'episode073', 'episode074', 'episode075', 'episode076', 'episode077', 'episode078', 'episode079', 'episode080', 'episode081', 'episode082', 'episode083', 'episode084', 'episode085', 'episode086', 'episode087', 'episode088', 'episode089', 'episode090', 'episode091', 'episode092', 'episode093', 'episode094', 'episode095', 'episode096', 'episode097', 'episode098', 'episode099', 'episode100', 'episode101', 'episode102', 'episode103', 'episode104', 'episode105', 'episode106', 'episode107', 'episode108']\n"
     ]
    }
   ],
   "source": [
    "with np.load(\"/AILAB-summer-school-2025/dataset_all.npz\") as f:\n",
    "    print(f.files)  # 어떤 episode까지 있는지\n",
    "    arr = f[\"episode019\"]  # 에러 전까진 열림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf0301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"/AILAB-summer-school-2025/dataset_all.npz\") as f:\n",
    "    for k in f.files:\n",
    "        try:\n",
    "            _ = f[k]   # 실제로 로드 시도\n",
    "        except Exception as e:\n",
    "            print(\"에러:\", k, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94493c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(NpzFile '/AILAB-summer-school-2025/simulation_traj_2_20250829_121225_len498_success/robot_state.npz' with keys: EE_pose, obs, applied_torque)\n",
      "EE_pose (346, 7) float32\n",
      "obs (346, 29) float32\n",
      "applied_torque (0,) float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 파일 로드\n",
    "data = np.load(\"/AILAB-summer-school-2025/simulation_traj_2_20250829_121225_len498_success/robot_state.npz\")\n",
    "\n",
    "# 키 목록 출력\n",
    "print(\"Keys:\", data.keys())\n",
    "\n",
    "# 각 key별 value의 shape와 dtype 확인\n",
    "for k in data.keys():\n",
    "    print(k, data[k].shape, data[k].dtype)\n",
    "# for i in range(10):\n",
    "#     print(data['episode000'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf54987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(NpzFile 'dataset_all_afterpregrasp_t3.npz' with keys: episode000, episode001, episode002, episode003, episode004...)\n",
      "[ 3.0280006e-01 -5.6915607e-02  6.2759602e-01  2.3422081e-07\n",
      "  1.0000000e+00 -2.2864535e-06  3.6135559e-06 -1.0000000e+00\n",
      "  4.1523328e-01  1.6660001e-02  5.0053000e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 파일 로드\n",
    "data = np.load(\"dataset_all_afterpregrasp_t3.npz\")\n",
    "\n",
    "# 키 목록 출력\n",
    "print(\"Keys:\", data.keys())\n",
    "\n",
    "# 각 key별 value의 shape와 dtype 확인\n",
    "# for k in data.keys():\n",
    "#     print(k, data[k].shape, data[k].dtype)\n",
    "for i in range(1):\n",
    "    print(data['episode000'][i])\n",
    "\n",
    "len(data['episode003'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67e01358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 스텝: 58376\n",
      "col00: min=0.154143  max=0.493000  mean=0.334388  std=0.092651\n",
      "col01: min=-0.210957  max=0.616374  mean=0.100874  std=0.299396\n",
      "col02: min=0.016626  max=0.627596  mean=0.283371  std=0.200197\n",
      "col03: min=-0.309463  max=0.296086  mean=-0.014204  std=0.061968\n",
      "col04: min=-0.996798  max=1.000000  mean=0.927372  std=0.326956\n",
      "col05: min=-0.915723  max=0.493847  mean=0.066481  std=0.125248\n",
      "col06: min=-0.280613  max=0.425593  mean=0.041707  std=0.085204\n",
      "col07: min=-1.000000  max=1.000000  mean=-0.259216  std=0.965635\n",
      "col08: min=0.116535  max=0.493333  mean=0.364519  std=0.091515\n",
      "col09: min=-0.096853  max=0.697277  mean=0.187702  std=0.286137\n",
      "col10: min=0.500529  max=1.041341  mean=0.661926  std=0.188267\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"dataset_all_afterpregrasp_t3.npz\"\n",
    "npz = np.load(path, allow_pickle=True)\n",
    "\n",
    "keys = sorted([k for k in npz.files if k.startswith(\"episode\")])\n",
    "if not keys:\n",
    "    raise ValueError(\"episode### 키가 없습니다.\")\n",
    "\n",
    "rows = []\n",
    "for k in keys:\n",
    "    a = np.asarray(npz[k])\n",
    "    if a.ndim == 1 and a.shape[0] == 11:\n",
    "        a = a[None, :]\n",
    "    if a.ndim != 2:\n",
    "        raise ValueError(f\"{k} shape 이상: {a.shape}\")\n",
    "    if a.shape[1] != 11:\n",
    "        if a.T.shape[1] == 11:\n",
    "            a = a.T\n",
    "        else:\n",
    "            raise ValueError(f\"{k} 열이 11이 아님: {a.shape}\")\n",
    "    rows.append(a)\n",
    "\n",
    "data = np.vstack(rows)          # (total_steps, 11)\n",
    "\n",
    "mins  = np.min(data, axis=0)\n",
    "maxs  = np.max(data, axis=0)\n",
    "means = np.mean(data, axis=0)\n",
    "std = np.std(data, axis = 0)\n",
    "print(f\"총 스텝: {data.shape[0]}\")\n",
    "for i in range(11):\n",
    "    print(f\"col{i:02d}: min={mins[i]:.6f}  max={maxs[i]:.6f}  mean={means[i]:.6f}  std={std[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3271b3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001  val_mse 1.317846e-02\n",
      "epoch 002  val_mse 9.391064e-03\n",
      "epoch 003  val_mse 8.913240e-03\n",
      "epoch 004  val_mse 7.919356e-03\n",
      "epoch 005  val_mse 8.014124e-03\n",
      "epoch 006  val_mse 7.391402e-03\n",
      "epoch 007  val_mse 7.514128e-03\n",
      "epoch 008  val_mse 7.453711e-03\n",
      "epoch 009  val_mse 7.438667e-03\n",
      "epoch 010  val_mse 7.281705e-03\n",
      "epoch 011  val_mse 7.476663e-03\n",
      "epoch 012  val_mse 7.157443e-03\n",
      "epoch 013  val_mse 7.689284e-03\n",
      "epoch 014  val_mse 7.476829e-03\n",
      "epoch 015  val_mse 7.330987e-03\n",
      "epoch 016  val_mse 7.008285e-03\n",
      "epoch 017  val_mse 7.046681e-03\n",
      "epoch 018  val_mse 6.895128e-03\n",
      "epoch 019  val_mse 6.979250e-03\n",
      "epoch 020  val_mse 6.951074e-03\n",
      "epoch 021  val_mse 7.034390e-03\n",
      "epoch 022  val_mse 7.122997e-03\n",
      "epoch 023  val_mse 6.895843e-03\n",
      "epoch 024  val_mse 7.037154e-03\n",
      "epoch 025  val_mse 7.011926e-03\n",
      "epoch 026  val_mse 6.847736e-03\n",
      "epoch 027  val_mse 7.056269e-03\n",
      "epoch 028  val_mse 6.736265e-03\n",
      "epoch 029  val_mse 6.763923e-03\n",
      "epoch 030  val_mse 6.921984e-03\n",
      "epoch 031  val_mse 6.737926e-03\n",
      "epoch 032  val_mse 6.644336e-03\n",
      "epoch 033  val_mse 7.333722e-03\n",
      "epoch 034  val_mse 6.676173e-03\n",
      "epoch 035  val_mse 6.689753e-03\n",
      "epoch 036  val_mse 7.452159e-03\n",
      "epoch 037  val_mse 6.451261e-03\n",
      "epoch 038  val_mse 7.042587e-03\n",
      "epoch 039  val_mse 6.550990e-03\n",
      "epoch 040  val_mse 6.331922e-03\n",
      "epoch 041  val_mse 6.605886e-03\n",
      "epoch 042  val_mse 6.656420e-03\n",
      "epoch 043  val_mse 6.953543e-03\n",
      "epoch 044  val_mse 6.408886e-03\n",
      "epoch 045  val_mse 6.133090e-03\n",
      "epoch 046  val_mse 6.616864e-03\n",
      "epoch 047  val_mse 6.339637e-03\n",
      "epoch 048  val_mse 6.469597e-03\n",
      "epoch 049  val_mse 6.281842e-03\n",
      "epoch 050  val_mse 6.263616e-03\n",
      "epoch 051  val_mse 6.111377e-03\n",
      "epoch 052  val_mse 6.269896e-03\n",
      "epoch 053  val_mse 5.961057e-03\n",
      "epoch 054  val_mse 6.544750e-03\n",
      "epoch 055  val_mse 5.981776e-03\n",
      "epoch 056  val_mse 6.082576e-03\n",
      "epoch 057  val_mse 6.666177e-03\n",
      "epoch 058  val_mse 6.428341e-03\n",
      "epoch 059  val_mse 6.595103e-03\n",
      "epoch 060  val_mse 6.555087e-03\n",
      "epoch 061  val_mse 5.817586e-03\n",
      "epoch 062  val_mse 5.992071e-03\n",
      "epoch 063  val_mse 6.164357e-03\n",
      "epoch 064  val_mse 5.683474e-03\n",
      "epoch 065  val_mse 6.032170e-03\n",
      "epoch 066  val_mse 6.375448e-03\n",
      "epoch 067  val_mse 5.922918e-03\n",
      "epoch 068  val_mse 6.494626e-03\n",
      "epoch 069  val_mse 6.350655e-03\n",
      "epoch 070  val_mse 6.396174e-03\n",
      "epoch 071  val_mse 5.755096e-03\n",
      "epoch 072  val_mse 6.530840e-03\n",
      "epoch 073  val_mse 5.533555e-03\n",
      "epoch 074  val_mse 5.857893e-03\n",
      "epoch 075  val_mse 6.057911e-03\n",
      "epoch 076  val_mse 6.154347e-03\n",
      "epoch 077  val_mse 6.190815e-03\n",
      "epoch 078  val_mse 6.099838e-03\n",
      "epoch 079  val_mse 5.958773e-03\n",
      "epoch 080  val_mse 5.353258e-03\n",
      "epoch 081  val_mse 5.825580e-03\n",
      "epoch 082  val_mse 6.186905e-03\n",
      "epoch 083  val_mse 6.425283e-03\n",
      "epoch 084  val_mse 6.624537e-03\n",
      "epoch 085  val_mse 5.639262e-03\n",
      "epoch 086  val_mse 5.577470e-03\n",
      "epoch 087  val_mse 5.900323e-03\n",
      "epoch 088  val_mse 5.409359e-03\n",
      "epoch 089  val_mse 6.311247e-03\n",
      "epoch 090  val_mse 5.802787e-03\n",
      "epoch 091  val_mse 6.208676e-03\n",
      "epoch 092  val_mse 5.202965e-03\n",
      "epoch 093  val_mse 5.746669e-03\n",
      "epoch 094  val_mse 5.433802e-03\n",
      "epoch 095  val_mse 5.649388e-03\n",
      "epoch 096  val_mse 5.682140e-03\n",
      "epoch 097  val_mse 5.544792e-03\n",
      "epoch 098  val_mse 5.296434e-03\n",
      "epoch 099  val_mse 5.671497e-03\n",
      "epoch 100  val_mse 5.786785e-03\n",
      "epoch 101  val_mse 5.775813e-03\n",
      "epoch 102  val_mse 5.517000e-03\n",
      "epoch 103  val_mse 5.752312e-03\n",
      "epoch 104  val_mse 5.570646e-03\n",
      "epoch 105  val_mse 6.020569e-03\n",
      "epoch 106  val_mse 5.345939e-03\n",
      "epoch 107  val_mse 5.976659e-03\n",
      "epoch 108  val_mse 5.277016e-03\n",
      "epoch 109  val_mse 5.341939e-03\n",
      "epoch 110  val_mse 5.706392e-03\n",
      "epoch 111  val_mse 6.004009e-03\n",
      "epoch 112  val_mse 5.514828e-03\n",
      "epoch 113  val_mse 5.143508e-03\n",
      "epoch 114  val_mse 5.926492e-03\n",
      "epoch 115  val_mse 5.306858e-03\n",
      "epoch 116  val_mse 5.801575e-03\n",
      "epoch 117  val_mse 5.613222e-03\n",
      "epoch 118  val_mse 6.090671e-03\n",
      "epoch 119  val_mse 5.841638e-03\n",
      "epoch 120  val_mse 5.259421e-03\n",
      "epoch 121  val_mse 5.759508e-03\n",
      "epoch 122  val_mse 6.647643e-03\n",
      "epoch 123  val_mse 6.025141e-03\n",
      "epoch 124  val_mse 5.838841e-03\n",
      "epoch 125  val_mse 5.552175e-03\n",
      "epoch 126  val_mse 5.471200e-03\n",
      "epoch 127  val_mse 5.578185e-03\n",
      "epoch 128  val_mse 5.903080e-03\n",
      "epoch 129  val_mse 5.865577e-03\n",
      "epoch 130  val_mse 6.003882e-03\n",
      "epoch 131  val_mse 5.168040e-03\n",
      "epoch 132  val_mse 6.052457e-03\n",
      "epoch 133  val_mse 5.944247e-03\n",
      "epoch 134  val_mse 5.355265e-03\n",
      "epoch 135  val_mse 5.998105e-03\n",
      "epoch 136  val_mse 6.941566e-03\n",
      "epoch 137  val_mse 5.507943e-03\n",
      "epoch 138  val_mse 5.709485e-03\n",
      "epoch 139  val_mse 5.117552e-03\n",
      "epoch 140  val_mse 6.527134e-03\n",
      "epoch 141  val_mse 5.958244e-03\n",
      "epoch 142  val_mse 6.233547e-03\n",
      "epoch 143  val_mse 5.665790e-03\n",
      "epoch 144  val_mse 6.450194e-03\n",
      "epoch 145  val_mse 5.613394e-03\n",
      "epoch 146  val_mse 6.352577e-03\n",
      "epoch 147  val_mse 5.751863e-03\n",
      "epoch 148  val_mse 5.576311e-03\n",
      "epoch 149  val_mse 5.649436e-03\n",
      "epoch 150  val_mse 6.449625e-03\n",
      "epoch 151  val_mse 4.995186e-03\n",
      "epoch 152  val_mse 4.943259e-03\n",
      "epoch 153  val_mse 5.639226e-03\n",
      "epoch 154  val_mse 5.533297e-03\n",
      "epoch 155  val_mse 5.409421e-03\n",
      "epoch 156  val_mse 5.687450e-03\n",
      "epoch 157  val_mse 6.914678e-03\n",
      "epoch 158  val_mse 5.673642e-03\n",
      "epoch 159  val_mse 5.169290e-03\n",
      "epoch 160  val_mse 5.679283e-03\n",
      "epoch 161  val_mse 5.536481e-03\n",
      "epoch 162  val_mse 5.891049e-03\n",
      "epoch 163  val_mse 5.101619e-03\n",
      "epoch 164  val_mse 5.932553e-03\n",
      "epoch 165  val_mse 5.494172e-03\n",
      "epoch 166  val_mse 5.021964e-03\n",
      "epoch 167  val_mse 5.370236e-03\n",
      "epoch 168  val_mse 5.006508e-03\n",
      "epoch 169  val_mse 5.952821e-03\n",
      "epoch 170  val_mse 5.691342e-03\n",
      "epoch 171  val_mse 6.298082e-03\n",
      "epoch 172  val_mse 6.458303e-03\n",
      "epoch 173  val_mse 7.767768e-03\n",
      "epoch 174  val_mse 5.933403e-03\n",
      "epoch 175  val_mse 6.308389e-03\n",
      "epoch 176  val_mse 5.838315e-03\n",
      "epoch 177  val_mse 5.742208e-03\n",
      "epoch 178  val_mse 5.362121e-03\n",
      "epoch 179  val_mse 5.049386e-03\n",
      "epoch 180  val_mse 5.779328e-03\n",
      "epoch 181  val_mse 5.902735e-03\n",
      "epoch 182  val_mse 5.624256e-03\n",
      "epoch 183  val_mse 6.060304e-03\n",
      "epoch 184  val_mse 5.400282e-03\n",
      "epoch 185  val_mse 6.099108e-03\n",
      "epoch 186  val_mse 5.879234e-03\n",
      "epoch 187  val_mse 5.278597e-03\n",
      "epoch 188  val_mse 5.613480e-03\n",
      "epoch 189  val_mse 7.850557e-03\n",
      "epoch 190  val_mse 6.081118e-03\n",
      "epoch 191  val_mse 6.050564e-03\n",
      "epoch 192  val_mse 5.027348e-03\n",
      "epoch 193  val_mse 5.169975e-03\n",
      "epoch 194  val_mse 6.251508e-03\n",
      "epoch 195  val_mse 5.537404e-03\n",
      "epoch 196  val_mse 5.836413e-03\n",
      "epoch 197  val_mse 5.189736e-03\n",
      "epoch 198  val_mse 5.357483e-03\n",
      "epoch 199  val_mse 5.371294e-03\n",
      "epoch 200  val_mse 5.571274e-03\n",
      "epoch 201  val_mse 5.260157e-03\n",
      "epoch 202  val_mse 5.746766e-03\n",
      "epoch 203  val_mse 5.450810e-03\n",
      "epoch 204  val_mse 6.078804e-03\n",
      "epoch 205  val_mse 6.674632e-03\n",
      "epoch 206  val_mse 5.924582e-03\n",
      "epoch 207  val_mse 6.411141e-03\n",
      "epoch 208  val_mse 6.366880e-03\n",
      "epoch 209  val_mse 7.192981e-03\n",
      "epoch 210  val_mse 5.917288e-03\n",
      "epoch 211  val_mse 5.387911e-03\n",
      "epoch 212  val_mse 6.540559e-03\n",
      "epoch 213  val_mse 4.876174e-03\n",
      "epoch 214  val_mse 5.863245e-03\n",
      "epoch 215  val_mse 5.575176e-03\n",
      "epoch 216  val_mse 5.459659e-03\n",
      "epoch 217  val_mse 5.109089e-03\n",
      "epoch 218  val_mse 5.052023e-03\n",
      "epoch 219  val_mse 5.681027e-03\n",
      "epoch 220  val_mse 5.049119e-03\n",
      "epoch 221  val_mse 5.115333e-03\n",
      "epoch 222  val_mse 4.932417e-03\n",
      "epoch 223  val_mse 5.675718e-03\n",
      "epoch 224  val_mse 5.373929e-03\n",
      "epoch 225  val_mse 6.302477e-03\n",
      "epoch 226  val_mse 5.751990e-03\n",
      "epoch 227  val_mse 7.095722e-03\n",
      "epoch 228  val_mse 5.493459e-03\n",
      "epoch 229  val_mse 5.347259e-03\n",
      "epoch 230  val_mse 4.825608e-03\n",
      "epoch 231  val_mse 5.428485e-03\n",
      "epoch 232  val_mse 6.304224e-03\n",
      "epoch 233  val_mse 6.531789e-03\n",
      "epoch 234  val_mse 5.901692e-03\n",
      "epoch 235  val_mse 6.157454e-03\n",
      "epoch 236  val_mse 5.608139e-03\n",
      "epoch 237  val_mse 5.408419e-03\n",
      "epoch 238  val_mse 5.661470e-03\n",
      "epoch 239  val_mse 5.517837e-03\n",
      "epoch 240  val_mse 6.038067e-03\n",
      "epoch 241  val_mse 5.653871e-03\n",
      "epoch 242  val_mse 5.506160e-03\n",
      "epoch 243  val_mse 5.555739e-03\n",
      "epoch 244  val_mse 8.260362e-03\n",
      "epoch 245  val_mse 8.553416e-03\n",
      "epoch 246  val_mse 5.225265e-03\n",
      "epoch 247  val_mse 5.127159e-03\n",
      "epoch 248  val_mse 6.048769e-03\n",
      "epoch 249  val_mse 5.222806e-03\n",
      "epoch 250  val_mse 5.862149e-03\n",
      "epoch 251  val_mse 5.589594e-03\n",
      "epoch 252  val_mse 6.963745e-03\n",
      "epoch 253  val_mse 4.961808e-03\n",
      "epoch 254  val_mse 5.315251e-03\n",
      "epoch 255  val_mse 5.762449e-03\n",
      "epoch 256  val_mse 5.890577e-03\n",
      "epoch 257  val_mse 5.410686e-03\n",
      "epoch 258  val_mse 6.321402e-03\n",
      "epoch 259  val_mse 6.154192e-03\n",
      "epoch 260  val_mse 5.377953e-03\n",
      "epoch 261  val_mse 5.429412e-03\n",
      "epoch 262  val_mse 6.819625e-03\n",
      "epoch 263  val_mse 6.651883e-03\n",
      "epoch 264  val_mse 6.571681e-03\n",
      "epoch 265  val_mse 6.161201e-03\n",
      "epoch 266  val_mse 6.240361e-03\n",
      "epoch 267  val_mse 6.026071e-03\n",
      "epoch 268  val_mse 5.883918e-03\n",
      "epoch 269  val_mse 6.233027e-03\n",
      "epoch 270  val_mse 5.362619e-03\n",
      "epoch 271  val_mse 5.634856e-03\n",
      "epoch 272  val_mse 4.857116e-03\n",
      "epoch 273  val_mse 6.142072e-03\n",
      "epoch 274  val_mse 5.706222e-03\n",
      "epoch 275  val_mse 5.138142e-03\n",
      "epoch 276  val_mse 8.083504e-03\n",
      "epoch 277  val_mse 8.356553e-03\n",
      "epoch 278  val_mse 6.856554e-03\n",
      "epoch 279  val_mse 5.564042e-03\n",
      "epoch 280  val_mse 6.333303e-03\n",
      "epoch 281  val_mse 5.843355e-03\n",
      "epoch 282  val_mse 5.490428e-03\n",
      "epoch 283  val_mse 7.339346e-03\n",
      "epoch 284  val_mse 6.746363e-03\n",
      "epoch 285  val_mse 5.958305e-03\n",
      "epoch 286  val_mse 6.832376e-03\n",
      "epoch 287  val_mse 5.138878e-03\n",
      "epoch 288  val_mse 5.662948e-03\n",
      "epoch 289  val_mse 5.720943e-03\n",
      "epoch 290  val_mse 6.372832e-03\n",
      "epoch 291  val_mse 6.193503e-03\n",
      "epoch 292  val_mse 6.135094e-03\n",
      "epoch 293  val_mse 7.419707e-03\n",
      "epoch 294  val_mse 6.814455e-03\n",
      "epoch 295  val_mse 6.616096e-03\n",
      "epoch 296  val_mse 6.709751e-03\n",
      "epoch 297  val_mse 5.743456e-03\n",
      "epoch 298  val_mse 7.073302e-03\n",
      "epoch 299  val_mse 6.486143e-03\n",
      "epoch 300  val_mse 5.311788e-03\n",
      "epoch 301  val_mse 7.456383e-03\n",
      "epoch 302  val_mse 5.319113e-03\n",
      "epoch 303  val_mse 6.027415e-03\n",
      "epoch 304  val_mse 6.956138e-03\n",
      "epoch 305  val_mse 6.267578e-03\n",
      "epoch 306  val_mse 6.386698e-03\n",
      "epoch 307  val_mse 5.803441e-03\n",
      "epoch 308  val_mse 6.454060e-03\n",
      "epoch 309  val_mse 6.428952e-03\n",
      "epoch 310  val_mse 6.460490e-03\n",
      "epoch 311  val_mse 5.683838e-03\n",
      "epoch 312  val_mse 5.479103e-03\n",
      "epoch 313  val_mse 5.455891e-03\n",
      "epoch 314  val_mse 8.481242e-03\n",
      "epoch 315  val_mse 6.380038e-03\n",
      "epoch 316  val_mse 7.461235e-03\n",
      "epoch 317  val_mse 6.365836e-03\n",
      "epoch 318  val_mse 9.049258e-03\n",
      "epoch 319  val_mse 5.866804e-03\n",
      "epoch 320  val_mse 6.205352e-03\n",
      "epoch 321  val_mse 5.834217e-03\n",
      "epoch 322  val_mse 6.279954e-03\n",
      "epoch 323  val_mse 6.611510e-03\n",
      "epoch 324  val_mse 7.279129e-03\n",
      "epoch 325  val_mse 6.922965e-03\n",
      "epoch 326  val_mse 4.997920e-03\n",
      "epoch 327  val_mse 6.967509e-03\n",
      "epoch 328  val_mse 6.060738e-03\n",
      "epoch 329  val_mse 8.926364e-03\n",
      "epoch 330  val_mse 7.336758e-03\n",
      "epoch 331  val_mse 6.752957e-03\n",
      "epoch 332  val_mse 5.914902e-03\n",
      "epoch 333  val_mse 5.603141e-03\n",
      "epoch 334  val_mse 7.767338e-03\n",
      "epoch 335  val_mse 6.544847e-03\n",
      "epoch 336  val_mse 7.197596e-03\n",
      "epoch 337  val_mse 5.756895e-03\n",
      "epoch 338  val_mse 8.365498e-03\n",
      "epoch 339  val_mse 6.903768e-03\n",
      "epoch 340  val_mse 5.638307e-03\n",
      "epoch 341  val_mse 6.313381e-03\n",
      "epoch 342  val_mse 6.008936e-03\n",
      "epoch 343  val_mse 6.209600e-03\n",
      "epoch 344  val_mse 8.058534e-03\n",
      "epoch 345  val_mse 6.079916e-03\n",
      "epoch 346  val_mse 5.647444e-03\n",
      "epoch 347  val_mse 5.860943e-03\n",
      "epoch 348  val_mse 5.537707e-03\n",
      "epoch 349  val_mse 5.126137e-03\n",
      "epoch 350  val_mse 5.600011e-03\n",
      "epoch 351  val_mse 6.608072e-03\n",
      "epoch 352  val_mse 5.351653e-03\n",
      "epoch 353  val_mse 6.050460e-03\n",
      "epoch 354  val_mse 5.520579e-03\n",
      "epoch 355  val_mse 6.066252e-03\n",
      "epoch 356  val_mse 5.970926e-03\n",
      "epoch 357  val_mse 4.916307e-03\n",
      "epoch 358  val_mse 7.170992e-03\n",
      "epoch 359  val_mse 6.642430e-03\n",
      "epoch 360  val_mse 9.085367e-03\n",
      "epoch 361  val_mse 5.956119e-03\n",
      "epoch 362  val_mse 8.059804e-03\n",
      "epoch 363  val_mse 4.728213e-03\n",
      "epoch 364  val_mse 6.893272e-03\n",
      "epoch 365  val_mse 6.093419e-03\n",
      "epoch 366  val_mse 5.769736e-03\n",
      "epoch 367  val_mse 6.936596e-03\n",
      "epoch 368  val_mse 5.848159e-03\n",
      "epoch 369  val_mse 5.721121e-03\n",
      "epoch 370  val_mse 5.779221e-03\n",
      "epoch 371  val_mse 5.495898e-03\n",
      "epoch 372  val_mse 7.372385e-03\n",
      "epoch 373  val_mse 9.696920e-03\n",
      "epoch 374  val_mse 5.574810e-03\n",
      "epoch 375  val_mse 7.062196e-03\n",
      "epoch 376  val_mse 6.371448e-03\n",
      "epoch 377  val_mse 5.571069e-03\n",
      "epoch 378  val_mse 6.082438e-03\n",
      "epoch 379  val_mse 5.881303e-03\n",
      "epoch 380  val_mse 6.194494e-03\n",
      "epoch 381  val_mse 7.281113e-03\n",
      "epoch 382  val_mse 6.239819e-03\n",
      "epoch 383  val_mse 6.998156e-03\n",
      "epoch 384  val_mse 5.782630e-03\n",
      "epoch 385  val_mse 5.365679e-03\n",
      "epoch 386  val_mse 6.326514e-03\n",
      "epoch 387  val_mse 5.521785e-03\n",
      "epoch 388  val_mse 5.803717e-03\n",
      "epoch 389  val_mse 5.761573e-03\n",
      "epoch 390  val_mse 6.079600e-03\n",
      "epoch 391  val_mse 6.209607e-03\n",
      "epoch 392  val_mse 5.773651e-03\n",
      "epoch 393  val_mse 6.399020e-03\n",
      "epoch 394  val_mse 6.043233e-03\n",
      "epoch 395  val_mse 7.464231e-03\n",
      "epoch 396  val_mse 5.856036e-03\n",
      "epoch 397  val_mse 8.293139e-03\n",
      "epoch 398  val_mse 5.973632e-03\n",
      "epoch 399  val_mse 7.272370e-03\n",
      "epoch 400  val_mse 7.312525e-03\n",
      "epoch 401  val_mse 5.111682e-03\n",
      "epoch 402  val_mse 7.384118e-03\n",
      "epoch 403  val_mse 7.018799e-03\n",
      "epoch 404  val_mse 6.886072e-03\n",
      "epoch 405  val_mse 7.702491e-03\n",
      "epoch 406  val_mse 8.338887e-03\n",
      "epoch 407  val_mse 5.490157e-03\n",
      "epoch 408  val_mse 5.101686e-03\n",
      "epoch 409  val_mse 5.568914e-03\n",
      "epoch 410  val_mse 7.285007e-03\n",
      "epoch 411  val_mse 7.634489e-03\n",
      "epoch 412  val_mse 6.237752e-03\n",
      "epoch 413  val_mse 7.446148e-03\n",
      "epoch 414  val_mse 5.550517e-03\n",
      "epoch 415  val_mse 6.614983e-03\n",
      "epoch 416  val_mse 7.442907e-03\n",
      "epoch 417  val_mse 7.400277e-03\n",
      "epoch 418  val_mse 5.365903e-03\n",
      "epoch 419  val_mse 1.033333e-02\n",
      "epoch 420  val_mse 6.152489e-03\n",
      "epoch 421  val_mse 7.796400e-03\n",
      "epoch 422  val_mse 6.086363e-03\n",
      "epoch 423  val_mse 6.282446e-03\n",
      "epoch 424  val_mse 7.299071e-03\n",
      "epoch 425  val_mse 5.753738e-03\n",
      "epoch 426  val_mse 6.633900e-03\n",
      "epoch 427  val_mse 5.403781e-03\n",
      "epoch 428  val_mse 6.663992e-03\n",
      "epoch 429  val_mse 6.350473e-03\n",
      "epoch 430  val_mse 6.199168e-03\n",
      "epoch 431  val_mse 5.457658e-03\n",
      "epoch 432  val_mse 5.341144e-03\n",
      "epoch 433  val_mse 5.964155e-03\n",
      "epoch 434  val_mse 7.216123e-03\n",
      "epoch 435  val_mse 6.244053e-03\n",
      "epoch 436  val_mse 7.095851e-03\n",
      "epoch 437  val_mse 5.284822e-03\n",
      "epoch 438  val_mse 4.701631e-03\n",
      "epoch 439  val_mse 6.801639e-03\n",
      "epoch 440  val_mse 5.644847e-03\n",
      "epoch 441  val_mse 5.346051e-03\n",
      "epoch 442  val_mse 6.427655e-03\n",
      "epoch 443  val_mse 7.171967e-03\n",
      "epoch 444  val_mse 6.074187e-03\n",
      "epoch 445  val_mse 5.712221e-03\n",
      "epoch 446  val_mse 4.679163e-03\n",
      "epoch 447  val_mse 5.437619e-03\n",
      "epoch 448  val_mse 4.670661e-03\n",
      "epoch 449  val_mse 6.139698e-03\n",
      "epoch 450  val_mse 5.770858e-03\n",
      "epoch 451  val_mse 5.083540e-03\n",
      "epoch 452  val_mse 5.474722e-03\n",
      "epoch 453  val_mse 5.086796e-03\n",
      "epoch 454  val_mse 5.036824e-03\n",
      "epoch 455  val_mse 4.676833e-03\n",
      "epoch 456  val_mse 6.662859e-03\n",
      "epoch 457  val_mse 5.990024e-03\n",
      "epoch 458  val_mse 4.790290e-03\n",
      "epoch 459  val_mse 4.535046e-03\n",
      "epoch 460  val_mse 4.926637e-03\n",
      "epoch 461  val_mse 5.529409e-03\n",
      "epoch 462  val_mse 5.374289e-03\n",
      "epoch 463  val_mse 5.816756e-03\n",
      "epoch 464  val_mse 4.573270e-03\n",
      "epoch 465  val_mse 5.039777e-03\n",
      "epoch 466  val_mse 5.090966e-03\n",
      "epoch 467  val_mse 5.080911e-03\n",
      "epoch 468  val_mse 4.890488e-03\n",
      "epoch 469  val_mse 7.006680e-03\n",
      "epoch 470  val_mse 4.746246e-03\n",
      "epoch 471  val_mse 5.647610e-03\n",
      "epoch 472  val_mse 4.652980e-03\n",
      "epoch 473  val_mse 4.320771e-03\n",
      "epoch 474  val_mse 4.775484e-03\n",
      "epoch 475  val_mse 6.790722e-03\n",
      "epoch 476  val_mse 4.918597e-03\n",
      "epoch 477  val_mse 5.087326e-03\n",
      "epoch 478  val_mse 4.572361e-03\n",
      "epoch 479  val_mse 5.583751e-03\n",
      "epoch 480  val_mse 5.292641e-03\n",
      "epoch 481  val_mse 5.550019e-03\n",
      "epoch 482  val_mse 5.662797e-03\n",
      "epoch 483  val_mse 5.848086e-03\n",
      "epoch 484  val_mse 5.397215e-03\n",
      "epoch 485  val_mse 4.783048e-03\n",
      "epoch 486  val_mse 5.733201e-03\n",
      "epoch 487  val_mse 4.840721e-03\n",
      "epoch 488  val_mse 5.391522e-03\n",
      "epoch 489  val_mse 5.657329e-03\n",
      "epoch 490  val_mse 5.327461e-03\n",
      "epoch 491  val_mse 6.183702e-03\n",
      "epoch 492  val_mse 5.166221e-03\n",
      "epoch 493  val_mse 4.843074e-03\n",
      "epoch 494  val_mse 4.614119e-03\n",
      "epoch 495  val_mse 4.986926e-03\n",
      "epoch 496  val_mse 4.949181e-03\n",
      "epoch 497  val_mse 4.712731e-03\n",
      "epoch 498  val_mse 4.406639e-03\n",
      "epoch 499  val_mse 4.474190e-03\n",
      "epoch 500  val_mse 5.298893e-03\n",
      "학습 완료. best val MSE: 0.004320770789810799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4019/1087357473.py:257: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"il_gru_best3.pt\", map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# Imitation Learning: s_{t-3:t-1} -> a_t(= s_t[:8])\n",
    "# - 입력: (3, 11), 출력: (8,)\n",
    "# - 초기 t<3 규칙: t=0,1,2 모두 복제 규칙 적용\n",
    "# - 정규화: train split 통계로 표준화(입력 11, 출력 8 각각 별도)\n",
    "\n",
    "import os, json, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# 설정\n",
    "# -------------------------\n",
    "PATH = \"dataset_all_afterpregrasp_t3.npz\"\n",
    "VAL_RATIO = 0.1\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "LR = 1e-3\n",
    "HIDDEN = 128\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# 유틸\n",
    "# -------------------------\n",
    "def ensure_Tx11(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.ndim == 1 and a.shape[0] == 11:\n",
    "        return a[None, :]\n",
    "    if a.ndim == 2 and a.shape[1] == 11:\n",
    "        return a\n",
    "    if a.ndim == 2 and a.shape[0] == 11:\n",
    "        return a.T\n",
    "    raise ValueError(f\"shape must be (T,11) or (11,T), got {a.shape}\")\n",
    "\n",
    "def make_window(ep, t):\n",
    "    \"\"\"규칙대로 (3,11) 윈도우 생성. ep: (T,11)\"\"\"\n",
    "    if t == 0:\n",
    "        w = np.stack([ep[0], ep[0], ep[0]], axis=0)\n",
    "    elif t == 1:\n",
    "        w = np.stack([ep[0], ep[0], ep[0]], axis=0)  # 지시사항: 2번째 스텝 예측도 1번째 3번 복제\n",
    "    elif t == 2:\n",
    "        w = np.stack([ep[0], ep[1], ep[1]], axis=0)  # 지시사항: 1번째 1회 + 2번째 2회\n",
    "    else:\n",
    "        w = ep[t-3:t, :]\n",
    "    return w  # (3,11)\n",
    "\n",
    "class ColumnStandardizer:\n",
    "    def __init__(self, dim):\n",
    "        self.mu = np.zeros(dim, dtype=np.float64)\n",
    "        self.std = np.ones(dim, dtype=np.float64)\n",
    "\n",
    "    def fit(self, X):\n",
    "        # X: (N, dim) or (..., dim)\n",
    "        X2 = X.reshape(-1, X.shape[-1]).astype(np.float64)\n",
    "        self.mu = X2.mean(axis=0)\n",
    "        self.std = X2.std(axis=0)\n",
    "        self.std[self.std < 1e-8] = 1.0  # 분산 0 보호\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mu) / self.std\n",
    "\n",
    "    def inverse(self, Xn):\n",
    "        return Xn * self.std + self.mu\n",
    "\n",
    "    def state(self):\n",
    "        return {\"mu\": self.mu.tolist(), \"std\": self.std.tolist()}\n",
    "\n",
    "class ILDataset(Dataset):\n",
    "    def __init__(self, npz, keys, x_norm=None, y_norm=None):\n",
    "        self.samples = []  # list of (X[3,11], y[8])\n",
    "        for k in keys:\n",
    "            ep = ensure_Tx11(npz[k])\n",
    "            T = ep.shape[0]\n",
    "            for t in range(T):\n",
    "                # 타깃은 s_t[:8]\n",
    "                y = ep[t, :8]\n",
    "                X = make_window(ep, t)  # (3,11)\n",
    "                self.samples.append((X, y))\n",
    "        self.x_norm = x_norm\n",
    "        self.y_norm = y_norm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.samples[idx]\n",
    "        if self.x_norm is not None:\n",
    "            X = self.x_norm.transform(X)  # (3,11) 표준화\n",
    "        if self.y_norm is not None:\n",
    "            y = self.y_norm.transform(y)  # (8,)   표준화\n",
    "        X = torch.from_numpy(X.astype(np.float32))           # (3,11)\n",
    "        y = torch.from_numpy(y.astype(np.float32))           # (8,)\n",
    "        return X, y\n",
    "\n",
    "class GRUPolicy(nn.Module):\n",
    "    # 입력: (B,3,11) -> GRU -> head -> (B,8)\n",
    "    def __init__(self, in_dim=11, hidden=128, out_dim=8, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=in_dim, hidden_size=hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,3,11)\n",
    "        h, _ = self.gru(x)\n",
    "        last = h[:, -1, :]\n",
    "        return self.head(last)  # (B,8)\n",
    "\n",
    "class MLPPolicy(nn.Module):\n",
    "    # 입력: (B,3,11) -> flatten -> MLP -> (B,8)\n",
    "    def __init__(self, in_dim=11, win=3, hidden=256, out_dim=8):\n",
    "        super().__init__()\n",
    "        self.win = win\n",
    "        self.in_dim = in_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim*win, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,3,11)\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, self.win * self.in_dim)  # (B,33)\n",
    "        return self.net(x)  # (B,8)\n",
    "\n",
    "# -------------------------\n",
    "# 데이터 로드 및 split\n",
    "# -------------------------\n",
    "npz = np.load(PATH, allow_pickle=True)\n",
    "keys = sorted([k for k in npz.files if k.startswith(\"episode\")])\n",
    "if not keys:\n",
    "    raise RuntimeError(\"episode### 키가 없음\")\n",
    "\n",
    "# 에피소드 단위 분할\n",
    "n_val = max(1, int(len(keys) * VAL_RATIO))\n",
    "random.shuffle(keys)\n",
    "val_keys = sorted(keys[:n_val])\n",
    "train_keys = sorted(keys[n_val:])\n",
    "\n",
    "# 정규화 통계 추출을 위해 train 전부 펼치기\n",
    "train_states = []\n",
    "train_targets = []\n",
    "for k in train_keys:\n",
    "    ep = ensure_Tx11(npz[k])\n",
    "    T = ep.shape[0]\n",
    "    # 입력용: 윈도우 3개 모두 포함하므로, 먼저 원본 분포 기준으로 통계 계산\n",
    "    # 입력 정규화는 state 컬럼별 통계 필요 → 원본 ep 전 구간 사용\n",
    "    train_states.append(ep)           # (T,11)\n",
    "    train_targets.append(ep[:, :8])   # (T,8)\n",
    "\n",
    "train_states = np.concatenate(train_states, axis=0)  # (S,11)\n",
    "train_targets = np.concatenate(train_targets, axis=0)  # (S,8)\n",
    "\n",
    "x_norm = ColumnStandardizer(dim=11)\n",
    "y_norm = ColumnStandardizer(dim=8)\n",
    "x_norm.fit(train_states)\n",
    "y_norm.fit(train_targets)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_ds = ILDataset(npz, train_keys, x_norm, y_norm)\n",
    "val_ds   = ILDataset(npz, val_keys,   x_norm, y_norm)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "# -------------------------\n",
    "# 학습 루프\n",
    "# -------------------------\n",
    "model = GRUPolicy(in_dim=11, hidden=HIDDEN, out_dim=8).to(DEVICE)\n",
    "# model = MLPPolicy(in_dim=11, win=3, hidden=256, out_dim=8).to(DEVICE)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    tot, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(DEVICE)   # (B,3,11)\n",
    "            y = y.to(DEVICE)   # (B,8)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            bs = X.size(0)\n",
    "            tot += loss.item() * bs\n",
    "            n += bs\n",
    "    return tot / max(n, 1)\n",
    "\n",
    "best_val = math.inf\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(DEVICE)   # (B,3,11)\n",
    "        y = y.to(DEVICE)   # (B,8)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    val_mse = evaluate()\n",
    "    print(f\"epoch {epoch:03d}  val_mse {val_mse:.6e}\")\n",
    "    if val_mse < best_val:\n",
    "        best_val = val_mse\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"x_norm\": x_norm.state(),\n",
    "                    \"y_norm\": y_norm.state()}, \"il_gru_best3.pt\")\n",
    "\n",
    "print(\"학습 완료. best val MSE:\", best_val)\n",
    "\n",
    "# -------------------------\n",
    "# 추론 함수 예시\n",
    "# -------------------------\n",
    "def load_norm(state):\n",
    "    sd_x = np.array(state[\"x_norm\"][\"mu\"], dtype=np.float64)\n",
    "    sd_y = np.array(state[\"y_norm\"][\"mu\"], dtype=np.float64)  # not used here\n",
    "    # 이미 ColumnStandardizer 상태 저장했으므로 그대로 복원\n",
    "    nx = ColumnStandardizer(11)\n",
    "    ny = ColumnStandardizer(8)\n",
    "    nx.mu = np.array(state[\"x_norm\"][\"mu\"], dtype=np.float64)\n",
    "    nx.std = np.array(state[\"x_norm\"][\"std\"], dtype=np.float64)\n",
    "    ny.mu = np.array(state[\"y_norm\"][\"mu\"], dtype=np.float64)\n",
    "    ny.std = np.array(state[\"y_norm\"][\"std\"], dtype=np.float64)\n",
    "    return nx, ny\n",
    "\n",
    "def predict_action(model, x_norm, y_norm, history_ks):\n",
    "    \"\"\"\n",
    "    history_ks: numpy array of shape (k,11), k in {1,2,3}\n",
    "    규칙에 맞춰 (3,11)로 만들고 정규화하여 예측. 반환은 비정규화된 (8,)\n",
    "    \"\"\"\n",
    "    hist = ensure_Tx11(history_ks)  # (k,11)\n",
    "    if hist.shape[0] == 1:\n",
    "        X = np.stack([hist[0], hist[0], hist[0]], axis=0)\n",
    "    elif hist.shape[0] == 2:\n",
    "        X = np.stack([hist[0], hist[1], hist[1]], axis=0)\n",
    "    else:\n",
    "        X = hist[-3:, :]\n",
    "    Xn = x_norm.transform(X).astype(np.float32)\n",
    "    xt = torch.from_numpy(Xn)[None, ...].to(DEVICE)  # (1,3,11)\n",
    "    with torch.no_grad():\n",
    "        yn = model(xt).cpu().numpy()[0]  # (8,)\n",
    "    y = y_norm.inverse(yn)\n",
    "    return y  # (8,)\n",
    "\n",
    "# 저장된 베스트 체크포인트 복원 예시\n",
    "ckpt = torch.load(\"il_gru_best3.pt\", map_location=DEVICE)\n",
    "\n",
    "# model = MLPPolicy(in_dim=11, win=3, hidden=256, out_dim=8).to(DEVICE)\n",
    "# model.load_state_dict(ckpt[\"model\"])\n",
    "model = GRUPolicy(in_dim=11, hidden=HIDDEN, out_dim=8).to(DEVICE)  # GRU로 생성\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "x_norm_re, y_norm_re = load_norm({\n",
    "    \"x_norm\": ckpt[\"x_norm\"],\n",
    "    \"y_norm\": ckpt[\"y_norm\"],\n",
    "})\n",
    "\n",
    "# 예시 입력으로 동작 확인\n",
    "# hist = np.array([\n",
    "#   [0.2955, -0.0532, 0.1080, -0.0028, 1.0029, -0.0139, -0.0057, -1.0000, 0.1029, -0.2139, -0.0357],\n",
    "#   [0.2890, -0.1274, 0.0823,  0.0184, 0.9880, -0.0451,  0.0118, -1.0000, 0.0229, -0.7131, -0.3056],\n",
    "#   [0.2899, -0.1282, 0.0969,  0.0253, 0.9847, -0.0472,  0.0422,  1.0000, -0.0249, -1.0109, -0.0057],\n",
    "# ], dtype=np.float32)\n",
    "# act8 = predict_action(model, x_norm_re, y_norm_re, hist)\n",
    "# print(\"pred action(8):\", act8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c24f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred action(8): [ 0.30003545 -0.09169847  0.25209087  0.01513988  1.00816255  0.05462717\n",
      "  0.07752625  1.12046635]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 예시 입력으로 동작 확인\n",
    "hist = np.array([\n",
    "  [ 0.3047, -0.0590, 0.2224 ,  0.0041,  0.9997,  0.0149,  0.0179,  1.0000, 0.3047, -0.0590, 0.02198],\n",
    "  [0.3028, -0.0620,  0.2207,  0.0081,  0.9987,  0.0268,  0.0416,  1.0000, 0.3047, -0.0590, 0.0198],\n",
    "  [0.3001, -0.0655,  0.2198,  0.0122,  0.9970,  0.0366,  0.0671,  1.0000, 0.3047, -0.0590, 0.0198],\n",
    "], dtype=np.float32)\n",
    "act8 = predict_action(model, x_norm_re, y_norm_re, hist)\n",
    "print(\"pred action(8):\", act8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63e7c380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001  val_mse 5.671916e-03\n",
      "epoch 002  val_mse 4.695730e-03\n",
      "epoch 003  val_mse 4.459597e-03\n",
      "epoch 004  val_mse 4.447963e-03\n",
      "epoch 005  val_mse 4.359194e-03\n",
      "epoch 006  val_mse 4.268745e-03\n",
      "epoch 007  val_mse 4.327085e-03\n",
      "epoch 008  val_mse 4.318000e-03\n",
      "epoch 009  val_mse 4.245312e-03\n",
      "epoch 010  val_mse 4.306353e-03\n",
      "epoch 011  val_mse 4.216555e-03\n",
      "epoch 012  val_mse 4.214160e-03\n",
      "epoch 013  val_mse 4.332897e-03\n",
      "epoch 014  val_mse 4.307450e-03\n",
      "epoch 015  val_mse 4.308227e-03\n",
      "epoch 016  val_mse 4.372952e-03\n",
      "epoch 017  val_mse 4.269401e-03\n",
      "epoch 018  val_mse 4.233909e-03\n",
      "epoch 019  val_mse 4.157946e-03\n",
      "epoch 020  val_mse 4.204786e-03\n",
      "epoch 021  val_mse 4.333825e-03\n",
      "epoch 022  val_mse 4.365305e-03\n",
      "epoch 023  val_mse 4.222906e-03\n",
      "epoch 024  val_mse 4.229270e-03\n",
      "epoch 025  val_mse 4.140822e-03\n",
      "epoch 026  val_mse 4.100462e-03\n",
      "epoch 027  val_mse 4.271703e-03\n",
      "epoch 028  val_mse 4.164424e-03\n",
      "epoch 029  val_mse 4.108528e-03\n",
      "epoch 030  val_mse 4.165260e-03\n",
      "epoch 031  val_mse 4.070407e-03\n",
      "epoch 032  val_mse 4.170882e-03\n",
      "epoch 033  val_mse 4.221653e-03\n",
      "epoch 034  val_mse 4.106551e-03\n",
      "epoch 035  val_mse 4.103782e-03\n",
      "epoch 036  val_mse 4.326689e-03\n",
      "epoch 037  val_mse 4.111934e-03\n",
      "epoch 038  val_mse 4.292418e-03\n",
      "epoch 039  val_mse 4.128811e-03\n",
      "epoch 040  val_mse 4.029282e-03\n",
      "epoch 041  val_mse 4.154684e-03\n",
      "epoch 042  val_mse 4.198968e-03\n",
      "epoch 043  val_mse 4.063473e-03\n",
      "epoch 044  val_mse 4.004282e-03\n",
      "epoch 045  val_mse 4.017930e-03\n",
      "epoch 046  val_mse 4.060878e-03\n",
      "epoch 047  val_mse 4.055220e-03\n",
      "epoch 048  val_mse 4.026107e-03\n",
      "epoch 049  val_mse 3.938346e-03\n",
      "epoch 050  val_mse 4.067829e-03\n",
      "epoch 051  val_mse 3.917991e-03\n",
      "epoch 052  val_mse 4.145777e-03\n",
      "epoch 053  val_mse 3.913341e-03\n",
      "epoch 054  val_mse 4.210596e-03\n",
      "epoch 055  val_mse 3.952980e-03\n",
      "epoch 056  val_mse 3.987472e-03\n",
      "epoch 057  val_mse 3.938393e-03\n",
      "epoch 058  val_mse 4.118457e-03\n",
      "epoch 059  val_mse 3.865751e-03\n",
      "epoch 060  val_mse 3.975023e-03\n",
      "epoch 061  val_mse 3.919132e-03\n",
      "epoch 062  val_mse 3.884276e-03\n",
      "epoch 063  val_mse 4.036037e-03\n",
      "epoch 064  val_mse 3.871934e-03\n",
      "epoch 065  val_mse 4.137190e-03\n",
      "epoch 066  val_mse 4.100353e-03\n",
      "epoch 067  val_mse 4.079114e-03\n",
      "epoch 068  val_mse 4.015446e-03\n",
      "epoch 069  val_mse 3.990384e-03\n",
      "epoch 070  val_mse 4.123170e-03\n",
      "epoch 071  val_mse 3.968285e-03\n",
      "epoch 072  val_mse 4.245031e-03\n",
      "epoch 073  val_mse 3.760467e-03\n",
      "epoch 074  val_mse 3.792598e-03\n",
      "epoch 075  val_mse 3.858721e-03\n",
      "epoch 076  val_mse 4.180922e-03\n",
      "epoch 077  val_mse 3.838094e-03\n",
      "epoch 078  val_mse 4.012966e-03\n",
      "epoch 079  val_mse 4.088642e-03\n",
      "epoch 080  val_mse 3.696195e-03\n",
      "epoch 081  val_mse 4.029785e-03\n",
      "epoch 082  val_mse 4.017662e-03\n",
      "epoch 083  val_mse 4.156394e-03\n",
      "epoch 084  val_mse 4.109899e-03\n",
      "epoch 085  val_mse 3.947448e-03\n",
      "epoch 086  val_mse 4.450220e-03\n",
      "epoch 087  val_mse 4.053673e-03\n",
      "epoch 088  val_mse 3.674810e-03\n",
      "epoch 089  val_mse 4.498151e-03\n",
      "epoch 090  val_mse 4.202841e-03\n",
      "epoch 091  val_mse 3.949897e-03\n",
      "epoch 092  val_mse 3.713356e-03\n",
      "epoch 093  val_mse 4.194780e-03\n",
      "epoch 094  val_mse 3.887528e-03\n",
      "epoch 095  val_mse 3.939209e-03\n",
      "epoch 096  val_mse 3.600113e-03\n",
      "epoch 097  val_mse 3.581796e-03\n",
      "epoch 098  val_mse 3.592955e-03\n",
      "epoch 099  val_mse 4.152392e-03\n",
      "epoch 100  val_mse 4.072332e-03\n",
      "epoch 101  val_mse 4.022210e-03\n",
      "epoch 102  val_mse 3.712367e-03\n",
      "epoch 103  val_mse 3.507845e-03\n",
      "epoch 104  val_mse 3.909150e-03\n",
      "epoch 105  val_mse 3.514867e-03\n",
      "epoch 106  val_mse 3.921121e-03\n",
      "epoch 107  val_mse 3.681408e-03\n",
      "epoch 108  val_mse 3.779282e-03\n",
      "epoch 109  val_mse 3.610685e-03\n",
      "epoch 110  val_mse 3.676031e-03\n",
      "epoch 111  val_mse 3.780050e-03\n",
      "epoch 112  val_mse 3.619367e-03\n",
      "epoch 113  val_mse 3.461199e-03\n",
      "epoch 114  val_mse 4.153688e-03\n",
      "epoch 115  val_mse 3.727588e-03\n",
      "epoch 116  val_mse 3.989885e-03\n",
      "epoch 117  val_mse 3.555502e-03\n",
      "epoch 118  val_mse 3.745963e-03\n",
      "epoch 119  val_mse 3.410754e-03\n",
      "epoch 120  val_mse 3.487378e-03\n",
      "epoch 121  val_mse 3.398124e-03\n",
      "epoch 122  val_mse 3.710324e-03\n",
      "epoch 123  val_mse 3.753780e-03\n",
      "epoch 124  val_mse 3.954636e-03\n",
      "epoch 125  val_mse 3.603386e-03\n",
      "epoch 126  val_mse 3.740498e-03\n",
      "epoch 127  val_mse 3.999386e-03\n",
      "epoch 128  val_mse 3.381014e-03\n",
      "epoch 129  val_mse 3.954303e-03\n",
      "epoch 130  val_mse 3.900741e-03\n",
      "epoch 131  val_mse 3.506755e-03\n",
      "epoch 132  val_mse 3.881127e-03\n",
      "epoch 133  val_mse 3.831242e-03\n",
      "epoch 134  val_mse 3.871910e-03\n",
      "epoch 135  val_mse 4.023971e-03\n",
      "epoch 136  val_mse 3.958095e-03\n",
      "epoch 137  val_mse 3.751017e-03\n",
      "epoch 138  val_mse 3.356198e-03\n",
      "epoch 139  val_mse 3.395950e-03\n",
      "epoch 140  val_mse 3.486600e-03\n",
      "epoch 141  val_mse 3.819179e-03\n",
      "epoch 142  val_mse 3.771573e-03\n",
      "epoch 143  val_mse 3.314494e-03\n",
      "epoch 144  val_mse 3.493967e-03\n",
      "epoch 145  val_mse 3.783257e-03\n",
      "epoch 146  val_mse 4.100043e-03\n",
      "epoch 147  val_mse 3.790654e-03\n",
      "epoch 148  val_mse 3.830069e-03\n",
      "epoch 149  val_mse 4.047129e-03\n",
      "epoch 150  val_mse 4.030073e-03\n",
      "epoch 151  val_mse 3.219379e-03\n",
      "epoch 152  val_mse 3.820367e-03\n",
      "epoch 153  val_mse 3.626275e-03\n",
      "epoch 154  val_mse 3.598263e-03\n",
      "epoch 155  val_mse 3.364054e-03\n",
      "epoch 156  val_mse 3.382719e-03\n",
      "epoch 157  val_mse 3.570003e-03\n",
      "epoch 158  val_mse 3.302369e-03\n",
      "epoch 159  val_mse 3.275887e-03\n",
      "epoch 160  val_mse 3.599371e-03\n",
      "epoch 161  val_mse 3.630350e-03\n",
      "epoch 162  val_mse 3.931284e-03\n",
      "epoch 163  val_mse 3.794092e-03\n",
      "epoch 164  val_mse 3.914231e-03\n",
      "epoch 165  val_mse 3.443163e-03\n",
      "epoch 166  val_mse 3.190323e-03\n",
      "epoch 167  val_mse 3.164405e-03\n",
      "epoch 168  val_mse 3.952011e-03\n",
      "epoch 169  val_mse 3.357789e-03\n",
      "epoch 170  val_mse 3.680978e-03\n",
      "epoch 171  val_mse 3.877470e-03\n",
      "epoch 172  val_mse 3.220588e-03\n",
      "epoch 173  val_mse 3.364564e-03\n",
      "epoch 174  val_mse 3.814676e-03\n",
      "epoch 175  val_mse 3.352536e-03\n",
      "epoch 176  val_mse 3.341189e-03\n",
      "epoch 177  val_mse 3.248170e-03\n",
      "epoch 178  val_mse 3.272880e-03\n",
      "epoch 179  val_mse 3.406512e-03\n",
      "epoch 180  val_mse 3.550124e-03\n",
      "epoch 181  val_mse 3.181003e-03\n",
      "epoch 182  val_mse 3.880896e-03\n",
      "epoch 183  val_mse 3.922501e-03\n",
      "epoch 184  val_mse 3.096440e-03\n",
      "epoch 185  val_mse 3.386211e-03\n",
      "epoch 186  val_mse 3.478544e-03\n",
      "epoch 187  val_mse 3.402803e-03\n",
      "epoch 188  val_mse 3.291745e-03\n",
      "epoch 189  val_mse 3.301824e-03\n",
      "epoch 190  val_mse 3.593441e-03\n",
      "epoch 191  val_mse 3.190426e-03\n",
      "epoch 192  val_mse 3.322730e-03\n",
      "epoch 193  val_mse 3.790729e-03\n",
      "epoch 194  val_mse 3.764135e-03\n",
      "epoch 195  val_mse 3.848587e-03\n",
      "epoch 196  val_mse 3.794420e-03\n",
      "epoch 197  val_mse 3.234815e-03\n",
      "epoch 198  val_mse 3.220896e-03\n",
      "epoch 199  val_mse 3.199518e-03\n",
      "epoch 200  val_mse 3.182423e-03\n",
      "epoch 201  val_mse 3.192906e-03\n",
      "epoch 202  val_mse 3.209237e-03\n",
      "epoch 203  val_mse 3.305393e-03\n",
      "epoch 204  val_mse 3.624714e-03\n",
      "epoch 205  val_mse 3.621172e-03\n",
      "epoch 206  val_mse 3.230092e-03\n",
      "epoch 207  val_mse 3.774105e-03\n",
      "epoch 208  val_mse 3.721978e-03\n",
      "epoch 209  val_mse 3.491113e-03\n",
      "epoch 210  val_mse 4.015506e-03\n",
      "epoch 211  val_mse 3.672322e-03\n",
      "epoch 212  val_mse 3.221866e-03\n",
      "epoch 213  val_mse 3.433358e-03\n",
      "epoch 214  val_mse 3.759964e-03\n",
      "epoch 215  val_mse 3.329562e-03\n",
      "epoch 216  val_mse 3.051713e-03\n",
      "epoch 217  val_mse 3.088754e-03\n",
      "epoch 218  val_mse 3.408071e-03\n",
      "epoch 219  val_mse 3.284476e-03\n",
      "epoch 220  val_mse 3.073738e-03\n",
      "epoch 221  val_mse 3.038841e-03\n",
      "epoch 222  val_mse 3.125727e-03\n",
      "epoch 223  val_mse 3.279212e-03\n",
      "epoch 224  val_mse 3.218778e-03\n",
      "epoch 225  val_mse 3.237942e-03\n",
      "epoch 226  val_mse 3.172606e-03\n",
      "epoch 227  val_mse 3.840443e-03\n",
      "epoch 228  val_mse 3.243868e-03\n",
      "epoch 229  val_mse 3.159153e-03\n",
      "epoch 230  val_mse 3.436541e-03\n",
      "epoch 231  val_mse 3.869311e-03\n",
      "epoch 232  val_mse 2.970231e-03\n",
      "epoch 233  val_mse 3.926629e-03\n",
      "epoch 234  val_mse 3.462530e-03\n",
      "epoch 235  val_mse 3.817501e-03\n",
      "epoch 236  val_mse 3.421074e-03\n",
      "epoch 237  val_mse 3.188796e-03\n",
      "epoch 238  val_mse 3.125164e-03\n",
      "epoch 239  val_mse 3.625400e-03\n",
      "epoch 240  val_mse 3.502710e-03\n",
      "epoch 241  val_mse 3.487992e-03\n",
      "epoch 242  val_mse 3.093441e-03\n",
      "epoch 243  val_mse 2.956678e-03\n",
      "epoch 244  val_mse 3.747985e-03\n",
      "epoch 245  val_mse 3.174567e-03\n",
      "epoch 246  val_mse 3.430991e-03\n",
      "epoch 247  val_mse 3.861555e-03\n",
      "epoch 248  val_mse 3.843113e-03\n",
      "epoch 249  val_mse 3.316133e-03\n",
      "epoch 250  val_mse 3.236063e-03\n",
      "epoch 251  val_mse 3.365581e-03\n",
      "epoch 252  val_mse 3.155241e-03\n",
      "epoch 253  val_mse 3.101154e-03\n",
      "epoch 254  val_mse 3.664365e-03\n",
      "epoch 255  val_mse 3.254801e-03\n",
      "epoch 256  val_mse 3.795539e-03\n",
      "epoch 257  val_mse 3.091656e-03\n",
      "epoch 258  val_mse 3.164759e-03\n",
      "epoch 259  val_mse 3.328193e-03\n",
      "epoch 260  val_mse 3.224236e-03\n",
      "epoch 261  val_mse 3.353243e-03\n",
      "epoch 262  val_mse 3.295547e-03\n",
      "epoch 263  val_mse 3.209823e-03\n",
      "epoch 264  val_mse 3.342262e-03\n",
      "epoch 265  val_mse 3.503522e-03\n",
      "epoch 266  val_mse 3.146997e-03\n",
      "epoch 267  val_mse 3.085153e-03\n",
      "epoch 268  val_mse 3.134716e-03\n",
      "epoch 269  val_mse 3.388213e-03\n",
      "epoch 270  val_mse 2.861008e-03\n",
      "epoch 271  val_mse 3.031361e-03\n",
      "epoch 272  val_mse 3.129263e-03\n",
      "epoch 273  val_mse 3.870576e-03\n",
      "epoch 274  val_mse 3.161156e-03\n",
      "epoch 275  val_mse 3.214546e-03\n",
      "epoch 276  val_mse 3.756479e-03\n",
      "epoch 277  val_mse 2.933684e-03\n",
      "epoch 278  val_mse 3.395818e-03\n",
      "epoch 279  val_mse 3.963952e-03\n",
      "epoch 280  val_mse 2.965819e-03\n",
      "epoch 281  val_mse 2.918762e-03\n",
      "epoch 282  val_mse 3.503534e-03\n",
      "epoch 283  val_mse 3.657129e-03\n",
      "epoch 284  val_mse 3.176437e-03\n",
      "epoch 285  val_mse 3.116851e-03\n",
      "epoch 286  val_mse 4.243744e-03\n",
      "epoch 287  val_mse 3.578786e-03\n",
      "epoch 288  val_mse 3.989873e-03\n",
      "epoch 289  val_mse 3.157666e-03\n",
      "epoch 290  val_mse 3.444155e-03\n",
      "epoch 291  val_mse 3.499017e-03\n",
      "epoch 292  val_mse 3.052780e-03\n",
      "epoch 293  val_mse 3.516743e-03\n",
      "epoch 294  val_mse 3.610207e-03\n",
      "epoch 295  val_mse 3.879226e-03\n",
      "epoch 296  val_mse 3.750000e-03\n",
      "epoch 297  val_mse 3.591807e-03\n",
      "epoch 298  val_mse 3.342557e-03\n",
      "epoch 299  val_mse 3.408347e-03\n",
      "epoch 300  val_mse 3.254522e-03\n",
      "epoch 301  val_mse 3.934938e-03\n",
      "epoch 302  val_mse 3.196595e-03\n",
      "epoch 303  val_mse 3.135684e-03\n",
      "epoch 304  val_mse 2.995927e-03\n",
      "epoch 305  val_mse 2.895285e-03\n",
      "epoch 306  val_mse 3.583025e-03\n",
      "epoch 307  val_mse 3.202390e-03\n",
      "epoch 308  val_mse 2.955571e-03\n",
      "epoch 309  val_mse 2.840190e-03\n",
      "epoch 310  val_mse 3.217572e-03\n",
      "epoch 311  val_mse 3.568251e-03\n",
      "epoch 312  val_mse 3.518223e-03\n",
      "epoch 313  val_mse 2.804722e-03\n",
      "epoch 314  val_mse 3.190742e-03\n",
      "epoch 315  val_mse 3.841267e-03\n",
      "epoch 316  val_mse 3.920557e-03\n",
      "epoch 317  val_mse 2.866014e-03\n",
      "epoch 318  val_mse 3.375765e-03\n",
      "epoch 319  val_mse 2.904664e-03\n",
      "epoch 320  val_mse 3.314525e-03\n",
      "epoch 321  val_mse 2.990301e-03\n",
      "epoch 322  val_mse 3.091152e-03\n",
      "epoch 323  val_mse 3.041353e-03\n",
      "epoch 324  val_mse 3.446138e-03\n",
      "epoch 325  val_mse 3.718996e-03\n",
      "epoch 326  val_mse 3.072982e-03\n",
      "epoch 327  val_mse 2.876131e-03\n",
      "epoch 328  val_mse 3.180264e-03\n",
      "epoch 329  val_mse 3.321021e-03\n",
      "epoch 330  val_mse 3.275994e-03\n",
      "epoch 331  val_mse 3.773555e-03\n",
      "epoch 332  val_mse 3.624590e-03\n",
      "epoch 333  val_mse 3.245892e-03\n",
      "epoch 334  val_mse 3.115180e-03\n",
      "epoch 335  val_mse 3.288903e-03\n",
      "epoch 336  val_mse 2.901662e-03\n",
      "epoch 337  val_mse 3.261868e-03\n",
      "epoch 338  val_mse 2.955907e-03\n",
      "epoch 339  val_mse 4.259218e-03\n",
      "epoch 340  val_mse 3.048439e-03\n",
      "epoch 341  val_mse 3.660705e-03\n",
      "epoch 342  val_mse 3.297124e-03\n",
      "epoch 343  val_mse 2.981854e-03\n",
      "epoch 344  val_mse 3.379297e-03\n",
      "epoch 345  val_mse 3.191680e-03\n",
      "epoch 346  val_mse 2.950085e-03\n",
      "epoch 347  val_mse 3.081453e-03\n",
      "epoch 348  val_mse 3.403345e-03\n",
      "epoch 349  val_mse 3.231102e-03\n",
      "epoch 350  val_mse 3.086150e-03\n",
      "epoch 351  val_mse 3.890843e-03\n",
      "epoch 352  val_mse 3.713373e-03\n",
      "epoch 353  val_mse 3.199841e-03\n",
      "epoch 354  val_mse 3.166141e-03\n",
      "epoch 355  val_mse 3.366473e-03\n",
      "epoch 356  val_mse 3.705350e-03\n",
      "epoch 357  val_mse 3.237732e-03\n",
      "epoch 358  val_mse 3.186578e-03\n",
      "epoch 359  val_mse 3.389771e-03\n",
      "epoch 360  val_mse 3.192801e-03\n",
      "epoch 361  val_mse 3.042082e-03\n",
      "epoch 362  val_mse 3.047442e-03\n",
      "epoch 363  val_mse 3.200739e-03\n",
      "epoch 364  val_mse 3.732914e-03\n",
      "epoch 365  val_mse 3.686650e-03\n",
      "epoch 366  val_mse 3.199846e-03\n",
      "epoch 367  val_mse 3.563689e-03\n",
      "epoch 368  val_mse 3.205038e-03\n",
      "epoch 369  val_mse 3.817578e-03\n",
      "epoch 370  val_mse 2.979159e-03\n",
      "epoch 371  val_mse 2.860386e-03\n",
      "epoch 372  val_mse 3.014776e-03\n",
      "epoch 373  val_mse 3.536904e-03\n",
      "epoch 374  val_mse 3.411007e-03\n",
      "epoch 375  val_mse 3.454771e-03\n",
      "epoch 376  val_mse 3.605638e-03\n",
      "epoch 377  val_mse 3.089496e-03\n",
      "epoch 378  val_mse 3.318453e-03\n",
      "epoch 379  val_mse 3.146981e-03\n",
      "epoch 380  val_mse 3.273540e-03\n",
      "epoch 381  val_mse 4.145448e-03\n",
      "epoch 382  val_mse 3.393434e-03\n",
      "epoch 383  val_mse 3.272555e-03\n",
      "epoch 384  val_mse 3.790215e-03\n",
      "epoch 385  val_mse 3.233965e-03\n",
      "epoch 386  val_mse 3.403633e-03\n",
      "epoch 387  val_mse 3.654878e-03\n",
      "epoch 388  val_mse 3.172748e-03\n",
      "epoch 389  val_mse 3.412274e-03\n",
      "epoch 390  val_mse 4.063854e-03\n",
      "epoch 391  val_mse 3.510139e-03\n",
      "epoch 392  val_mse 3.119449e-03\n",
      "epoch 393  val_mse 3.319350e-03\n",
      "epoch 394  val_mse 3.371441e-03\n",
      "epoch 395  val_mse 3.392244e-03\n",
      "epoch 396  val_mse 3.083964e-03\n",
      "epoch 397  val_mse 3.362959e-03\n",
      "epoch 398  val_mse 3.249797e-03\n",
      "epoch 399  val_mse 3.422988e-03\n",
      "epoch 400  val_mse 3.192443e-03\n",
      "epoch 401  val_mse 3.699405e-03\n",
      "epoch 402  val_mse 3.442931e-03\n",
      "epoch 403  val_mse 3.264327e-03\n",
      "epoch 404  val_mse 3.311465e-03\n",
      "epoch 405  val_mse 3.079831e-03\n",
      "epoch 406  val_mse 3.105136e-03\n",
      "epoch 407  val_mse 3.708035e-03\n",
      "epoch 408  val_mse 3.720575e-03\n",
      "epoch 409  val_mse 3.054636e-03\n",
      "epoch 410  val_mse 3.769957e-03\n",
      "epoch 411  val_mse 3.735952e-03\n",
      "epoch 412  val_mse 3.536487e-03\n",
      "epoch 413  val_mse 3.314371e-03\n",
      "epoch 414  val_mse 3.258693e-03\n",
      "epoch 415  val_mse 3.176027e-03\n",
      "epoch 416  val_mse 3.693870e-03\n",
      "epoch 417  val_mse 3.188292e-03\n",
      "epoch 418  val_mse 3.975185e-03\n",
      "epoch 419  val_mse 3.064327e-03\n",
      "epoch 420  val_mse 3.389276e-03\n",
      "epoch 421  val_mse 3.004733e-03\n",
      "epoch 422  val_mse 3.521870e-03\n",
      "epoch 423  val_mse 3.388525e-03\n",
      "epoch 424  val_mse 3.359467e-03\n",
      "epoch 425  val_mse 2.898192e-03\n",
      "epoch 426  val_mse 3.738938e-03\n",
      "epoch 427  val_mse 3.292758e-03\n",
      "epoch 428  val_mse 3.008054e-03\n",
      "epoch 429  val_mse 3.045234e-03\n",
      "epoch 430  val_mse 4.012235e-03\n",
      "epoch 431  val_mse 3.101955e-03\n",
      "epoch 432  val_mse 3.147756e-03\n",
      "epoch 433  val_mse 3.426659e-03\n",
      "epoch 434  val_mse 3.514505e-03\n",
      "epoch 435  val_mse 2.833293e-03\n",
      "epoch 436  val_mse 3.017776e-03\n",
      "epoch 437  val_mse 3.389565e-03\n",
      "epoch 438  val_mse 3.116745e-03\n",
      "epoch 439  val_mse 3.296178e-03\n",
      "epoch 440  val_mse 3.385093e-03\n",
      "epoch 441  val_mse 3.203929e-03\n",
      "epoch 442  val_mse 3.546185e-03\n",
      "epoch 443  val_mse 3.345614e-03\n",
      "epoch 444  val_mse 3.142832e-03\n",
      "epoch 445  val_mse 2.974145e-03\n",
      "epoch 446  val_mse 3.413513e-03\n",
      "epoch 447  val_mse 4.228455e-03\n",
      "epoch 448  val_mse 3.166210e-03\n",
      "epoch 449  val_mse 3.548891e-03\n",
      "epoch 450  val_mse 2.892910e-03\n",
      "epoch 451  val_mse 2.807012e-03\n",
      "epoch 452  val_mse 3.003249e-03\n",
      "epoch 453  val_mse 3.652442e-03\n",
      "epoch 454  val_mse 3.367386e-03\n",
      "epoch 455  val_mse 3.175830e-03\n",
      "epoch 456  val_mse 3.917273e-03\n",
      "epoch 457  val_mse 3.645461e-03\n",
      "epoch 458  val_mse 3.711654e-03\n",
      "epoch 459  val_mse 3.172486e-03\n",
      "epoch 460  val_mse 3.480384e-03\n",
      "epoch 461  val_mse 2.846669e-03\n",
      "epoch 462  val_mse 2.971115e-03\n",
      "epoch 463  val_mse 3.875211e-03\n",
      "epoch 464  val_mse 3.100618e-03\n",
      "epoch 465  val_mse 3.967764e-03\n",
      "epoch 466  val_mse 3.535129e-03\n",
      "epoch 467  val_mse 3.321198e-03\n",
      "epoch 468  val_mse 3.454349e-03\n",
      "epoch 469  val_mse 4.062363e-03\n",
      "epoch 470  val_mse 3.060890e-03\n",
      "epoch 471  val_mse 3.827515e-03\n",
      "epoch 472  val_mse 3.426538e-03\n",
      "epoch 473  val_mse 3.919459e-03\n",
      "epoch 474  val_mse 3.088125e-03\n",
      "epoch 475  val_mse 3.463167e-03\n",
      "epoch 476  val_mse 3.075064e-03\n",
      "epoch 477  val_mse 2.991933e-03\n",
      "epoch 478  val_mse 3.083868e-03\n",
      "epoch 479  val_mse 3.239488e-03\n",
      "epoch 480  val_mse 3.450218e-03\n",
      "epoch 481  val_mse 3.109911e-03\n",
      "epoch 482  val_mse 3.294973e-03\n",
      "epoch 483  val_mse 3.511898e-03\n",
      "epoch 484  val_mse 3.263621e-03\n",
      "epoch 485  val_mse 3.012079e-03\n",
      "epoch 486  val_mse 3.185408e-03\n",
      "epoch 487  val_mse 3.081292e-03\n",
      "epoch 488  val_mse 3.039176e-03\n",
      "epoch 489  val_mse 3.337093e-03\n",
      "epoch 490  val_mse 3.505897e-03\n",
      "epoch 491  val_mse 3.688948e-03\n",
      "epoch 492  val_mse 3.208376e-03\n",
      "epoch 493  val_mse 3.616787e-03\n",
      "epoch 494  val_mse 2.733714e-03\n",
      "epoch 495  val_mse 2.912501e-03\n",
      "epoch 496  val_mse 3.937719e-03\n",
      "epoch 497  val_mse 3.034449e-03\n",
      "epoch 498  val_mse 2.860750e-03\n",
      "epoch 499  val_mse 3.264517e-03\n",
      "epoch 500  val_mse 3.138729e-03\n",
      "학습 완료. best val MSE: 0.0027337135403154235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4019/3829121068.py:244: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"il_gru_best4_nonnorm.pt\", map_location=DEVICE)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'il_gru_best_nonnorm.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 250\u001b[0m\n\u001b[1;32m    244\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mil_gru_best4_nonnorm.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# model = MLPPolicy(in_dim=11, win=3, hidden=256, out_dim=8).to(DEVICE)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(ckpt[\"model\"])\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# --- 체크포인트 로드 ---\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mil_gru_best_nonnorm.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m model \u001b[38;5;241m=\u001b[39m GRUPolicy(in_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, hidden\u001b[38;5;241m=\u001b[39mHIDDEN, out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    252\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'il_gru_best_nonnorm.pt'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# Imitation Learning: s_{t-3:t-1} -> a_t(= s_t[:8])\n",
    "# - 입력: (3, 11), 출력: (8,)\n",
    "# - 초기 t<3 규칙: t=0,1,2 모두 복제 규칙 적용\n",
    "# - 정규화: train split 통계로 표준화(입력 11, 출력 8 각각 별도)\n",
    "\n",
    "import os, json, random, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# 설정\n",
    "# -------------------------\n",
    "PATH = \"dataset_all_afterpregrasp_t3.npz\"\n",
    "VAL_RATIO = 0.1\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "LR = 1e-3\n",
    "HIDDEN = 128\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# 유틸\n",
    "# -------------------------\n",
    "def ensure_Tx11(arr):\n",
    "    a = np.asarray(arr)\n",
    "    if a.ndim == 1 and a.shape[0] == 11:\n",
    "        return a[None, :]\n",
    "    if a.ndim == 2 and a.shape[1] == 11:\n",
    "        return a\n",
    "    if a.ndim == 2 and a.shape[0] == 11:\n",
    "        return a.T\n",
    "    raise ValueError(f\"shape must be (T,11) or (11,T), got {a.shape}\")\n",
    "\n",
    "def make_window(ep, t):\n",
    "    \"\"\"규칙대로 (3,11) 윈도우 생성. ep: (T,11)\"\"\"\n",
    "    if t == 0:\n",
    "        w = np.stack([ep[0], ep[0], ep[0]], axis=0)\n",
    "    elif t == 1:\n",
    "        w = np.stack([ep[0], ep[0], ep[0]], axis=0)  # 지시사항: 2번째 스텝 예측도 1번째 3번 복제\n",
    "    elif t == 2:\n",
    "        w = np.stack([ep[0], ep[1], ep[1]], axis=0)  # 지시사항: 1번째 1회 + 2번째 2회\n",
    "    else:\n",
    "        w = ep[t-3:t, :]\n",
    "    return w  # (3,11)\n",
    "\n",
    "class ColumnStandardizer:\n",
    "    def __init__(self, dim):\n",
    "        self.mu = np.zeros(dim, dtype=np.float64)\n",
    "        self.std = np.ones(dim, dtype=np.float64)\n",
    "\n",
    "    def fit(self, X):\n",
    "        # X: (N, dim) or (..., dim)\n",
    "        X2 = X.reshape(-1, X.shape[-1]).astype(np.float64)\n",
    "        self.mu = X2.mean(axis=0)\n",
    "        self.std = X2.std(axis=0)\n",
    "        self.std[self.std < 1e-8] = 1.0  # 분산 0 보호\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mu) / self.std\n",
    "\n",
    "    def inverse(self, Xn):\n",
    "        return Xn * self.std + self.mu\n",
    "\n",
    "    def state(self):\n",
    "        return {\"mu\": self.mu.tolist(), \"std\": self.std.tolist()}\n",
    "\n",
    "class ILDataset(Dataset):\n",
    "    def __init__(self, npz, keys):\n",
    "        self.samples = []\n",
    "        for k in keys:\n",
    "            ep = ensure_Tx11(npz[k])\n",
    "            T = ep.shape[0]\n",
    "            for t in range(T):\n",
    "                y = ep[t, :8]\n",
    "                X = make_window(ep, t)\n",
    "                self.samples.append((X, y))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.samples[idx]\n",
    "        X = torch.from_numpy(X.astype(np.float32))  # (3,11)\n",
    "        y = torch.from_numpy(y.astype(np.float32))  # (8,)\n",
    "        return X, y\n",
    "\n",
    "class GRUPolicy(nn.Module):\n",
    "    # 입력: (B,3,11) -> GRU -> head -> (B,8)\n",
    "    def __init__(self, in_dim=11, hidden=128, out_dim=8, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=in_dim, hidden_size=hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,3,11)\n",
    "        h, _ = self.gru(x)\n",
    "        last = h[:, -1, :]\n",
    "        return self.head(last)  # (B,8)\n",
    "\n",
    "class MLPPolicy(nn.Module):\n",
    "    # 입력: (B,3,11) -> flatten -> MLP -> (B,8)\n",
    "    def __init__(self, in_dim=11, win=3, hidden=256, out_dim=8):\n",
    "        super().__init__()\n",
    "        self.win = win\n",
    "        self.in_dim = in_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim*win, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,3,11)\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, self.win * self.in_dim)  # (B,33)\n",
    "        return self.net(x)  # (B,8)\n",
    "\n",
    "# -------------------------\n",
    "# 데이터 로드 및 split\n",
    "# -------------------------\n",
    "npz = np.load(PATH, allow_pickle=True)\n",
    "keys = sorted([k for k in npz.files if k.startswith(\"episode\")])\n",
    "if not keys:\n",
    "    raise RuntimeError(\"episode### 키가 없음\")\n",
    "\n",
    "# 에피소드 단위 분할\n",
    "n_val = max(1, int(len(keys) * VAL_RATIO))\n",
    "random.shuffle(keys)\n",
    "val_keys = sorted(keys[:n_val])\n",
    "train_keys = sorted(keys[n_val:])\n",
    "\n",
    "# 정규화 통계 추출을 위해 train 전부 펼치기\n",
    "train_states = []\n",
    "train_targets = []\n",
    "for k in train_keys:\n",
    "    ep = ensure_Tx11(npz[k])\n",
    "    T = ep.shape[0]\n",
    "    # 입력용: 윈도우 3개 모두 포함하므로, 먼저 원본 분포 기준으로 통계 계산\n",
    "    # 입력 정규화는 state 컬럼별 통계 필요 → 원본 ep 전 구간 사용\n",
    "    train_states.append(ep)           # (T,11)\n",
    "    train_targets.append(ep[:, :8])   # (T,8)\n",
    "\n",
    "train_states = np.concatenate(train_states, axis=0)  # (S,11)\n",
    "train_targets = np.concatenate(train_targets, axis=0)  # (S,8)\n",
    "\n",
    "x_norm = ColumnStandardizer(dim=11)\n",
    "y_norm = ColumnStandardizer(dim=8)\n",
    "x_norm.fit(train_states)\n",
    "y_norm.fit(train_targets)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_ds = ILDataset(npz, train_keys)\n",
    "val_ds   = ILDataset(npz, val_keys)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "# -------------------------\n",
    "# 학습 루프\n",
    "# -------------------------\n",
    "model = GRUPolicy(in_dim=11, hidden=HIDDEN, out_dim=8).to(DEVICE)\n",
    "# model = MLPPolicy(in_dim=11, win=3, hidden=256, out_dim=8).to(DEVICE)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    tot, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(DEVICE)   # (B,3,11)\n",
    "            y = y.to(DEVICE)   # (B,8)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            bs = X.size(0)\n",
    "            tot += loss.item() * bs\n",
    "            n += bs\n",
    "    return tot / max(n, 1)\n",
    "\n",
    "best_val = math.inf\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(DEVICE)   # (B,3,11)\n",
    "        y = y.to(DEVICE)   # (B,8)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    val_mse = evaluate()\n",
    "    print(f\"epoch {epoch:03d}  val_mse {val_mse:.6e}\")\n",
    "    if val_mse < best_val:\n",
    "        best_val = val_mse\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"cfg\": {\"arch\": \"GRU\", \"hidden\": HIDDEN}  # 필요시 메타만 저장\n",
    "        }, \"il_gru_best4_nonnorm.pt\")\n",
    "\n",
    "print(\"학습 완료. best val MSE:\", best_val)\n",
    "\n",
    "# -------------------------\n",
    "# 추론 함수 예시\n",
    "# -------------------------\n",
    "def load_norm(state):\n",
    "    sd_x = np.array(state[\"x_norm\"][\"mu\"], dtype=np.float64)\n",
    "    sd_y = np.array(state[\"y_norm\"][\"mu\"], dtype=np.float64)  # not used here\n",
    "    # 이미 ColumnStandardizer 상태 저장했으므로 그대로 복원\n",
    "    nx = ColumnStandardizer(11)\n",
    "    ny = ColumnStandardizer(8)\n",
    "    nx.mu = np.array(state[\"x_norm\"][\"mu\"], dtype=np.float64)\n",
    "    nx.std = np.array(state[\"x_norm\"][\"std\"], dtype=np.float64)\n",
    "    ny.mu = np.array(state[\"y_norm\"][\"mu\"], dtype=np.float64)\n",
    "    ny.std = np.array(state[\"y_norm\"][\"std\"], dtype=np.float64)\n",
    "    return nx, ny\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_action(model, history_ks):\n",
    "    hist = ensure_Tx11(history_ks)\n",
    "    if hist.shape[0] == 1:\n",
    "        X = np.stack([hist[0], hist[0], hist[0]], axis=0)\n",
    "    elif hist.shape[0] == 2:\n",
    "        X = np.stack([hist[0], hist[1], hist[1]], axis=0)\n",
    "    else:\n",
    "        X = hist[-3:, :]\n",
    "\n",
    "    xt = torch.from_numpy(X.astype(np.float32))[None, ...].to(DEVICE)  # (1,3,11)\n",
    "    y  = model(xt)[0].detach().cpu().numpy()                            # (8,)\n",
    "    return y\n",
    "\n",
    "# 저장된 베스트 체크포인트 복원 예시\n",
    "ckpt = torch.load(\"il_gru_best4_nonnorm.pt\", map_location=DEVICE)\n",
    "\n",
    "# model = MLPPolicy(in_dim=11, win=3, hidden=256, out_dim=8).to(DEVICE)\n",
    "# model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "# --- 체크포인트 로드 ---\n",
    "ckpt = torch.load(\"il_gru_best_nonnorm.pt\", map_location=DEVICE, weights_only=True)\n",
    "model = GRUPolicy(in_dim=11, hidden=HIDDEN, out_dim=8).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 예시 입력으로 동작 확인\n",
    "# hist = np.array([\n",
    "#   [0.2955, -0.0532, 0.1080, -0.0028, 1.0029, -0.0139, -0.0057, -1.0000, 0.1029, -0.2139, -0.0357],\n",
    "#   [0.2890, -0.1274, 0.0823,  0.0184, 0.9880, -0.0451,  0.0118, -1.0000, 0.0229, -0.7131, -0.3056],\n",
    "#   [0.2899, -0.1282, 0.0969,  0.0253, 0.9847, -0.0472,  0.0422,  1.0000, -0.0249, -1.0109, -0.0057],\n",
    "# ], dtype=np.float32)\n",
    "# act8 = predict_action(model, x_norm_re, y_norm_re, hist)\n",
    "# print(\"pred action(8):\", act8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
